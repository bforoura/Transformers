{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyORuBQbKl2VWdfiBig+Jtrq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bforoura/Transformers/blob/main/nlp_transformers_ch6_ex3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jxS0FWEyFDGu"
      },
      "outputs": [],
      "source": [
        "#@title Step 1: Activate GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 2: Cloning the OpenAI GPT-2 Repository\n",
        "\n",
        "\n",
        "!git clone https://github.com/openai/gpt-2.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh06O2FsKxQg",
        "outputId": "89d70582-9a33-4424-f3fc-30c9915566cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 233, done.\u001b[K\n",
            "remote: Total 233 (delta 0), reused 0 (delta 0), pack-reused 233\u001b[K\n",
            "Receiving objects: 100% (233/233), 4.38 MiB | 15.46 MiB/s, done.\n",
            "Resolving deltas: 100% (124/124), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 3: Installing the requirements\n",
        "\n",
        "\n",
        "# import os          # when the VM restarts import os necessary\n",
        "# os.chdir(\"/content/gpt-2\")\n",
        "# !pip3 install -r requirements.txt\n",
        "\n",
        "# The requirements have been installed automatically.\n",
        "\n",
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.15.2\n",
        "\n",
        "#This notebook requires toposort, which is a topological sort algorithm:\n",
        "!pip install toposort\n"
      ],
      "metadata": {
        "id": "iZsjFx-LLTYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 4: Checking the Version of TensorFlow\n",
        "\n",
        "#Colab includes Tensorflow 2.x by default but we need a lower version\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtIklTBFMWzD",
        "outputId": "50701b23-fb7f-4ea7-f166-f89481d960d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 5: Downloading the 117M parameter GPT-2 Model\n",
        "\n",
        "# run code and send argument\n",
        "import os \n",
        "\n",
        "os.chdir(\"/content/gpt-2\")\n",
        "!python3 download_model.py '117M'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUPX95ZTMxg5",
        "outputId": "cb6ddcec-f681-47c3-c47d-64e3353661d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 1.05Mit/s]                                                     \n",
            "Fetching encoder.json: 1.04Mit [00:00, 5.51Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.06Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:11, 42.2Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 5.42Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 2.93Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 3.24Mit/s]                                                       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 6: Copying the Project Resources to src\n",
        "\n",
        "!cp /content/dset.txt /content/gpt-2/src/\n",
        "!cp -r /content/gpt-2/models/ /content/gpt-2/src/\n"
      ],
      "metadata": {
        "id": "N4Fw-c5XNpnP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 7: Copying the N Shepperd Training Files\n",
        "#Referfence GitHub repository: https://github.com/nshepperd/gpt-2\n",
        "\n",
        "import os # import after runtime is restarted\n",
        "\n",
        "!cp /content/train.py /content/gpt-2/src/\n",
        "!cp /content/load_dataset.py /content/gpt-2/src/\n",
        "!cp /content/encode.py /content/gpt-2/src/\n",
        "!cp /content/accumulate.py /content/gpt-2/src/\n",
        "!cp /content/memory_saving_gradients.py /content/gpt-2/src/\n"
      ],
      "metadata": {
        "id": "gGiTzqOuOeic"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 8: Encoding dataset\n",
        "\n",
        "import os # import after runtime is restarted\n",
        "\n",
        "os.chdir(\"/content/gpt-2/src/\")\n",
        "model_name=\"117M\"\n",
        "!python /content/gpt-2/src/encode.py dset.txt out.npz \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU2mwKriPmcO",
        "outputId": "60338cd2-3b7e-4be5-fbd3-865fcd49c360"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading files\n",
            "100% 1/1 [00:09<00:00,  9.12s/it]\n",
            "Writing out.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 9: Training the Model\n",
        "\n",
        "#Model saved after 1000 steps\n",
        "import os # import after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2/src/\")\n",
        "\n",
        "!python train.py --dataset out.npz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lST6tk4hR27y",
        "outputId": "370ab1dc-772b-40dd-f72b-bde60bd5a883"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:89: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:92: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2022-11-07 15:49:55.175411: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-11-07 15:49:55.178555: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
            "2022-11-07 15:49:55.178772: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x17759c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-11-07 15:49:55.178805: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "WARNING:tensorflow:From train.py:93: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From train.py:118: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:122: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:145: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:148: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:150: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:153: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:157: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "Loading checkpoint models/117M/model.ckpt\n",
            "Loading dataset...\n",
            "100% 1/1 [00:00<00:00, 11.94it/s]\n",
            "dataset has 2548152 tokens\n",
            "Training...\n",
            "[1 | 22.35] loss=3.44 avg=3.44\n",
            "[2 | 41.52] loss=2.99 avg=3.21\n",
            "[3 | 60.72] loss=3.30 avg=3.24\n",
            "[4 | 79.90] loss=3.20 avg=3.23\n",
            "[5 | 99.07] loss=3.59 avg=3.30\n",
            "[6 | 118.27] loss=3.39 avg=3.32\n",
            "[7 | 137.47] loss=3.30 avg=3.32\n",
            "[8 | 156.63] loss=3.57 avg=3.35\n",
            "[9 | 175.91] loss=2.77 avg=3.28\n",
            "[10 | 195.62] loss=3.08 avg=3.26\n",
            "[11 | 215.17] loss=3.11 avg=3.25\n",
            "[12 | 234.54] loss=3.00 avg=3.23\n",
            "[13 | 254.00] loss=3.37 avg=3.24\n",
            "[14 | 273.53] loss=3.16 avg=3.23\n",
            "[15 | 293.08] loss=3.22 avg=3.23\n",
            "[16 | 312.79] loss=3.12 avg=3.22\n",
            "[17 | 332.26] loss=3.16 avg=3.22\n",
            "[18 | 351.80] loss=3.09 avg=3.21\n",
            "[19 | 371.33] loss=3.24 avg=3.21\n",
            "[20 | 390.47] loss=3.24 avg=3.21\n",
            "[21 | 409.90] loss=3.27 avg=3.22\n",
            "[22 | 429.12] loss=3.03 avg=3.21\n",
            "[23 | 448.51] loss=3.22 avg=3.21\n",
            "[24 | 467.73] loss=3.53 avg=3.22\n",
            "[25 | 487.12] loss=3.01 avg=3.21\n",
            "[26 | 506.79] loss=3.31 avg=3.22\n",
            "[27 | 526.40] loss=3.25 avg=3.22\n",
            "[28 | 546.16] loss=3.22 avg=3.22\n",
            "[29 | 565.88] loss=3.12 avg=3.22\n",
            "[30 | 585.87] loss=3.33 avg=3.22\n",
            "[31 | 605.91] loss=3.02 avg=3.21\n",
            "[32 | 625.49] loss=3.21 avg=3.21\n",
            "[33 | 645.22] loss=3.03 avg=3.21\n",
            "[34 | 664.46] loss=3.09 avg=3.20\n",
            "[35 | 683.89] loss=2.95 avg=3.19\n",
            "[36 | 703.47] loss=3.05 avg=3.19\n",
            "[37 | 722.52] loss=3.35 avg=3.19\n",
            "[38 | 741.33] loss=3.26 avg=3.20\n",
            "[39 | 760.29] loss=3.01 avg=3.19\n",
            "[40 | 779.18] loss=3.19 avg=3.19\n",
            "[41 | 797.84] loss=2.61 avg=3.17\n",
            "[42 | 816.64] loss=3.29 avg=3.18\n",
            "[43 | 835.23] loss=2.52 avg=3.16\n",
            "[44 | 853.96] loss=3.10 avg=3.16\n",
            "[45 | 872.63] loss=3.28 avg=3.16\n",
            "[46 | 891.39] loss=3.18 avg=3.16\n",
            "[47 | 910.28] loss=3.19 avg=3.16\n",
            "[48 | 929.09] loss=3.06 avg=3.16\n",
            "[49 | 948.01] loss=3.17 avg=3.16\n",
            "[50 | 966.82] loss=3.27 avg=3.16\n",
            "[51 | 985.52] loss=3.09 avg=3.16\n",
            "[52 | 1004.52] loss=2.92 avg=3.15\n",
            "[53 | 1023.31] loss=3.26 avg=3.16\n",
            "[54 | 1042.10] loss=1.64 avg=3.12\n",
            "[55 | 1060.79] loss=2.88 avg=3.11\n",
            "[56 | 1079.44] loss=2.93 avg=3.11\n",
            "[57 | 1098.13] loss=3.27 avg=3.11\n",
            "[58 | 1116.77] loss=3.41 avg=3.12\n",
            "[59 | 1135.58] loss=3.04 avg=3.12\n",
            "[60 | 1154.35] loss=3.29 avg=3.12\n",
            "[61 | 1173.09] loss=2.94 avg=3.12\n",
            "[62 | 1191.91] loss=3.14 avg=3.12\n",
            "[63 | 1210.42] loss=3.27 avg=3.12\n",
            "[64 | 1229.06] loss=3.39 avg=3.13\n",
            "[65 | 1247.87] loss=3.37 avg=3.13\n",
            "[66 | 1266.55] loss=1.87 avg=3.11\n",
            "[67 | 1285.04] loss=2.54 avg=3.10\n",
            "[68 | 1303.72] loss=2.86 avg=3.09\n",
            "[69 | 1322.48] loss=3.08 avg=3.09\n",
            "[70 | 1341.39] loss=3.16 avg=3.09\n",
            "[71 | 1360.21] loss=2.75 avg=3.09\n",
            "[72 | 1379.50] loss=3.00 avg=3.08\n",
            "[73 | 1398.72] loss=3.46 avg=3.09\n",
            "[74 | 1418.45] loss=3.29 avg=3.09\n",
            "[75 | 1437.83] loss=2.98 avg=3.09\n",
            "[76 | 1456.83] loss=3.24 avg=3.10\n",
            "[77 | 1475.85] loss=3.08 avg=3.09\n",
            "[78 | 1495.38] loss=3.09 avg=3.09\n",
            "[79 | 1515.31] loss=3.13 avg=3.10\n",
            "[80 | 1535.39] loss=2.90 avg=3.09\n",
            "[81 | 1555.48] loss=3.02 avg=3.09\n",
            "[82 | 1575.00] loss=2.98 avg=3.09\n",
            "[83 | 1594.69] loss=2.94 avg=3.09\n",
            "[84 | 1614.66] loss=3.09 avg=3.09\n",
            "[85 | 1634.60] loss=2.78 avg=3.08\n",
            "[86 | 1654.03] loss=3.07 avg=3.08\n",
            "[87 | 1673.53] loss=2.91 avg=3.08\n",
            "[88 | 1692.96] loss=2.76 avg=3.07\n",
            "[89 | 1712.08] loss=3.63 avg=3.08\n",
            "[90 | 1731.41] loss=3.02 avg=3.08\n",
            "[91 | 1750.41] loss=2.86 avg=3.08\n",
            "[92 | 1769.80] loss=2.70 avg=3.07\n",
            "[93 | 1789.29] loss=3.24 avg=3.07\n",
            "[94 | 1808.79] loss=3.12 avg=3.07\n",
            "[95 | 1828.26] loss=3.50 avg=3.08\n",
            "[96 | 1847.73] loss=2.95 avg=3.08\n",
            "[97 | 1867.65] loss=2.88 avg=3.08\n",
            "[98 | 1887.66] loss=3.20 avg=3.08\n",
            "[99 | 1907.37] loss=3.00 avg=3.08\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"\n",
            "\n",
            "[100 | 2020.67] loss=2.88 avg=3.07\n",
            "[101 | 2040.18] loss=3.20 avg=3.08\n",
            "[102 | 2059.81] loss=2.86 avg=3.07\n",
            "[103 | 2079.25] loss=2.95 avg=3.07\n",
            "[104 | 2098.73] loss=3.00 avg=3.07\n",
            "[105 | 2118.23] loss=3.49 avg=3.08\n",
            "[106 | 2137.64] loss=3.15 avg=3.08\n",
            "[107 | 2157.07] loss=3.11 avg=3.08\n",
            "[108 | 2176.21] loss=2.93 avg=3.07\n",
            "[109 | 2195.28] loss=3.13 avg=3.08\n",
            "[110 | 2214.65] loss=3.07 avg=3.08\n",
            "[111 | 2233.92] loss=3.17 avg=3.08\n",
            "[112 | 2253.04] loss=2.76 avg=3.07\n",
            "[113 | 2272.04] loss=3.32 avg=3.08\n",
            "[114 | 2291.09] loss=3.21 avg=3.08\n",
            "[115 | 2310.37] loss=3.19 avg=3.08\n",
            "[116 | 2329.60] loss=2.90 avg=3.08\n",
            "[117 | 2348.91] loss=3.04 avg=3.08\n",
            "[118 | 2367.93] loss=2.98 avg=3.08\n",
            "[119 | 2387.29] loss=3.02 avg=3.07\n",
            "[120 | 2406.44] loss=3.04 avg=3.07\n",
            "[121 | 2425.48] loss=2.98 avg=3.07\n",
            "[122 | 2444.52] loss=2.95 avg=3.07\n",
            "[123 | 2463.73] loss=3.05 avg=3.07\n",
            "[124 | 2482.80] loss=3.03 avg=3.07\n",
            "[125 | 2501.68] loss=3.04 avg=3.07\n",
            "[126 | 2520.68] loss=3.45 avg=3.07\n",
            "[127 | 2539.87] loss=3.34 avg=3.08\n",
            "[128 | 2558.88] loss=3.06 avg=3.08\n",
            "[129 | 2577.88] loss=3.28 avg=3.08\n",
            "[130 | 2596.63] loss=2.29 avg=3.07\n",
            "[131 | 2615.35] loss=3.23 avg=3.07\n",
            "[132 | 2633.92] loss=3.32 avg=3.08\n",
            "[133 | 2652.80] loss=3.06 avg=3.08\n",
            "[134 | 2672.66] loss=2.82 avg=3.07\n",
            "[135 | 2692.51] loss=2.81 avg=3.07\n",
            "[136 | 2712.69] loss=3.05 avg=3.07\n",
            "[137 | 2732.24] loss=2.98 avg=3.07\n",
            "[138 | 2752.49] loss=3.29 avg=3.07\n",
            "[139 | 2772.66] loss=3.03 avg=3.07\n",
            "[140 | 2792.12] loss=2.89 avg=3.07\n",
            "[141 | 2812.61] loss=2.81 avg=3.06\n",
            "[142 | 2832.45] loss=3.03 avg=3.06\n",
            "[143 | 2853.16] loss=3.08 avg=3.06\n",
            "[144 | 2873.66] loss=3.30 avg=3.07\n",
            "[145 | 2893.92] loss=3.04 avg=3.07\n",
            "[146 | 2913.91] loss=3.10 avg=3.07\n",
            "[147 | 2933.31] loss=2.98 avg=3.07\n",
            "[148 | 2952.45] loss=3.00 avg=3.06\n",
            "[149 | 2971.95] loss=3.08 avg=3.06\n",
            "[150 | 2991.34] loss=3.17 avg=3.07\n",
            "[151 | 3010.98] loss=3.07 avg=3.07\n",
            "[152 | 3030.37] loss=3.06 avg=3.07\n",
            "[153 | 3049.59] loss=2.78 avg=3.06\n",
            "[154 | 3068.63] loss=3.09 avg=3.06\n",
            "[155 | 3087.88] loss=3.05 avg=3.06\n",
            "[156 | 3107.06] loss=2.86 avg=3.06\n",
            "[157 | 3126.39] loss=2.46 avg=3.05\n",
            "[158 | 3145.57] loss=3.41 avg=3.06\n",
            "[159 | 3164.60] loss=3.19 avg=3.06\n",
            "[160 | 3183.74] loss=2.95 avg=3.06\n",
            "[161 | 3202.95] loss=3.31 avg=3.06\n",
            "[162 | 3222.00] loss=2.84 avg=3.06\n",
            "[163 | 3241.35] loss=2.97 avg=3.06\n",
            "[164 | 3260.34] loss=2.94 avg=3.06\n",
            "[165 | 3279.46] loss=2.88 avg=3.05\n",
            "[166 | 3298.69] loss=3.14 avg=3.05\n",
            "[167 | 3317.77] loss=2.68 avg=3.05\n",
            "[168 | 3337.01] loss=2.77 avg=3.05\n",
            "[169 | 3358.62] loss=2.18 avg=3.04\n",
            "[170 | 3377.73] loss=3.24 avg=3.04\n",
            "[171 | 3396.82] loss=2.79 avg=3.03\n",
            "[172 | 3418.36] loss=2.87 avg=3.03\n",
            "[173 | 3437.63] loss=2.97 avg=3.03\n",
            "[174 | 3456.96] loss=2.86 avg=3.03\n",
            "[175 | 3476.25] loss=2.85 avg=3.03\n",
            "[176 | 3497.40] loss=2.92 avg=3.03\n",
            "[177 | 3516.65] loss=2.64 avg=3.02\n",
            "[178 | 3535.77] loss=2.97 avg=3.02\n",
            "[179 | 3557.48] loss=2.97 avg=3.02\n",
            "[180 | 3576.71] loss=3.08 avg=3.02\n",
            "[181 | 3595.90] loss=2.80 avg=3.02\n",
            "[182 | 3617.36] loss=3.02 avg=3.02\n",
            "[183 | 3636.63] loss=3.21 avg=3.02\n",
            "[184 | 3655.81] loss=3.04 avg=3.02\n",
            "[185 | 3675.60] loss=0.84 avg=3.00\n",
            "[186 | 3696.32] loss=2.76 avg=2.99\n",
            "[187 | 3715.37] loss=2.86 avg=2.99\n",
            "[188 | 3734.52] loss=3.00 avg=2.99\n",
            "[189 | 3753.81] loss=2.86 avg=2.99\n",
            "[190 | 3774.92] loss=2.83 avg=2.99\n",
            "[191 | 3794.14] loss=2.94 avg=2.99\n",
            "[192 | 3813.35] loss=3.05 avg=2.99\n",
            "[193 | 3834.32] loss=2.95 avg=2.99\n",
            "[194 | 3853.71] loss=3.20 avg=2.99\n",
            "[195 | 3872.94] loss=2.97 avg=2.99\n",
            "[196 | 3891.82] loss=2.82 avg=2.99\n",
            "[197 | 3912.88] loss=3.28 avg=2.99\n",
            "[198 | 3932.14] loss=3.15 avg=2.99\n",
            "[199 | 3951.12] loss=3.08 avg=2.99\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "proof of the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and\n",
            "\n",
            "[200 | 4060.81] loss=2.84 avg=2.99\n",
            "[201 | 4082.10] loss=3.12 avg=2.99\n",
            "[202 | 4101.09] loss=2.80 avg=2.99\n",
            "[203 | 4120.32] loss=3.11 avg=2.99\n",
            "[204 | 4141.50] loss=3.19 avg=3.00\n",
            "[205 | 4160.71] loss=3.20 avg=3.00\n",
            "[206 | 4179.95] loss=3.35 avg=3.00\n",
            "[207 | 4201.12] loss=2.97 avg=3.00\n",
            "[208 | 4220.31] loss=3.02 avg=3.00\n",
            "[209 | 4239.60] loss=3.02 avg=3.00\n",
            "[210 | 4260.01] loss=3.10 avg=3.00\n",
            "[211 | 4280.08] loss=2.92 avg=3.00\n",
            "[212 | 4299.17] loss=1.31 avg=2.98\n",
            "[213 | 4318.37] loss=2.25 avg=2.97\n",
            "[214 | 4339.61] loss=3.26 avg=2.98\n",
            "[215 | 4358.81] loss=2.99 avg=2.98\n",
            "[216 | 4378.05] loss=2.93 avg=2.98\n",
            "[217 | 4399.61] loss=3.13 avg=2.98\n",
            "[218 | 4418.89] loss=2.88 avg=2.98\n",
            "[219 | 4437.98] loss=3.12 avg=2.98\n",
            "[220 | 4459.10] loss=2.95 avg=2.98\n",
            "[221 | 4478.35] loss=3.07 avg=2.98\n",
            "[222 | 4497.68] loss=3.17 avg=2.98\n",
            "[223 | 4517.12] loss=2.66 avg=2.98\n",
            "[224 | 4538.26] loss=3.47 avg=2.98\n",
            "[225 | 4557.42] loss=2.91 avg=2.98\n",
            "[226 | 4576.57] loss=3.12 avg=2.98\n",
            "[227 | 4597.64] loss=3.54 avg=2.99\n",
            "[228 | 4616.91] loss=2.40 avg=2.98\n",
            "[229 | 4636.16] loss=3.03 avg=2.98\n",
            "[230 | 4657.59] loss=2.86 avg=2.98\n",
            "[231 | 4677.02] loss=3.17 avg=2.99\n",
            "[232 | 4696.25] loss=2.99 avg=2.99\n",
            "[233 | 4717.40] loss=2.41 avg=2.98\n",
            "[234 | 4736.53] loss=3.26 avg=2.98\n",
            "[235 | 4756.02] loss=3.15 avg=2.98\n",
            "[236 | 4775.94] loss=2.78 avg=2.98\n",
            "[237 | 4796.02] loss=3.02 avg=2.98\n",
            "[238 | 4815.15] loss=3.37 avg=2.99\n",
            "[239 | 4834.27] loss=2.87 avg=2.99\n",
            "[240 | 4855.42] loss=3.02 avg=2.99\n",
            "[241 | 4874.62] loss=2.90 avg=2.98\n",
            "[242 | 4894.00] loss=3.09 avg=2.99\n",
            "[243 | 4915.62] loss=3.16 avg=2.99\n",
            "[244 | 4935.37] loss=3.07 avg=2.99\n",
            "[245 | 4954.88] loss=2.88 avg=2.99\n",
            "[246 | 4976.74] loss=2.93 avg=2.99\n",
            "[247 | 4996.15] loss=3.18 avg=2.99\n",
            "[248 | 5015.52] loss=3.12 avg=2.99\n",
            "[249 | 5036.72] loss=3.10 avg=2.99\n",
            "[250 | 5056.10] loss=2.99 avg=2.99\n",
            "[251 | 5075.60] loss=3.13 avg=2.99\n",
            "[252 | 5094.91] loss=2.86 avg=2.99\n",
            "[253 | 5116.50] loss=2.79 avg=2.99\n",
            "[254 | 5136.08] loss=2.65 avg=2.99\n",
            "[255 | 5155.59] loss=2.77 avg=2.98\n",
            "[256 | 5174.84] loss=3.09 avg=2.98\n",
            "[257 | 5196.15] loss=2.76 avg=2.98\n",
            "[258 | 5215.68] loss=3.38 avg=2.99\n",
            "[259 | 5234.96] loss=3.21 avg=2.99\n",
            "[260 | 5256.48] loss=2.96 avg=2.99\n",
            "[261 | 5276.04] loss=2.78 avg=2.99\n",
            "[262 | 5295.67] loss=2.77 avg=2.98\n",
            "[263 | 5317.08] loss=3.32 avg=2.99\n",
            "[264 | 5336.53] loss=2.70 avg=2.98\n",
            "[265 | 5355.94] loss=3.01 avg=2.98\n",
            "[266 | 5377.47] loss=3.06 avg=2.99\n",
            "[267 | 5397.08] loss=3.04 avg=2.99\n",
            "[268 | 5416.92] loss=2.83 avg=2.98\n",
            "[269 | 5438.37] loss=3.15 avg=2.99\n",
            "[270 | 5457.89] loss=3.18 avg=2.99\n",
            "[271 | 5477.39] loss=2.73 avg=2.99\n",
            "[272 | 5496.84] loss=2.81 avg=2.98\n",
            "[273 | 5518.31] loss=2.79 avg=2.98\n",
            "[274 | 5537.58] loss=3.23 avg=2.98\n",
            "[275 | 5556.91] loss=2.43 avg=2.98\n",
            "[276 | 5578.07] loss=3.08 avg=2.98\n",
            "[277 | 5597.66] loss=3.38 avg=2.98\n",
            "[278 | 5617.16] loss=2.62 avg=2.98\n",
            "[279 | 5638.91] loss=2.91 avg=2.98\n",
            "[280 | 5658.34] loss=2.68 avg=2.98\n",
            "[281 | 5679.86] loss=3.14 avg=2.98\n",
            "[282 | 5701.70] loss=2.85 avg=2.98\n",
            "[283 | 5721.18] loss=3.19 avg=2.98\n",
            "[284 | 5740.62] loss=2.86 avg=2.98\n",
            "[285 | 5761.86] loss=2.85 avg=2.98\n",
            "[286 | 5781.35] loss=2.89 avg=2.98\n",
            "[287 | 5800.78] loss=2.70 avg=2.97\n",
            "[288 | 5821.37] loss=2.68 avg=2.97\n",
            "[289 | 5841.72] loss=2.82 avg=2.97\n",
            "[290 | 5861.38] loss=2.81 avg=2.97\n",
            "[291 | 5880.90] loss=3.06 avg=2.97\n",
            "[292 | 5902.44] loss=2.83 avg=2.97\n",
            "[293 | 5921.67] loss=2.93 avg=2.96\n",
            "[294 | 5943.10] loss=3.13 avg=2.97\n",
            "[295 | 5964.43] loss=2.93 avg=2.97\n",
            "[296 | 5984.04] loss=2.91 avg=2.97\n",
            "[297 | 6003.89] loss=3.40 avg=2.97\n",
            "[298 | 6025.88] loss=2.94 avg=2.97\n",
            "[299 | 6045.42] loss=2.98 avg=2.97\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " in the world, and the world is not a place of the mind, but a place of the mind, and the mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place\n",
            "\n",
            "[300 | 6157.91] loss=2.97 avg=2.97\n",
            "[301 | 6178.25] loss=3.09 avg=2.97\n",
            "[302 | 6198.71] loss=2.88 avg=2.97\n",
            "[303 | 6220.23] loss=2.82 avg=2.97\n",
            "[304 | 6239.65] loss=2.81 avg=2.97\n",
            "[305 | 6259.07] loss=2.97 avg=2.97\n",
            "[306 | 6280.84] loss=2.98 avg=2.97\n",
            "[307 | 6300.27] loss=3.25 avg=2.97\n",
            "[308 | 6319.59] loss=2.65 avg=2.97\n",
            "[309 | 6341.08] loss=2.96 avg=2.97\n",
            "[310 | 6360.71] loss=3.11 avg=2.97\n",
            "[311 | 6380.07] loss=2.86 avg=2.97\n",
            "[312 | 6401.31] loss=2.25 avg=2.96\n",
            "[313 | 6420.84] loss=2.79 avg=2.96\n",
            "[314 | 6442.54] loss=2.91 avg=2.96\n",
            "[315 | 6464.28] loss=2.85 avg=2.96\n",
            "[316 | 6483.53] loss=3.53 avg=2.96\n",
            "[317 | 6503.08] loss=2.97 avg=2.96\n",
            "[318 | 6522.92] loss=3.11 avg=2.96\n",
            "[319 | 6544.38] loss=3.16 avg=2.97\n",
            "[320 | 6563.78] loss=2.33 avg=2.96\n",
            "[321 | 6583.45] loss=3.08 avg=2.96\n",
            "[322 | 6604.68] loss=2.98 avg=2.96\n",
            "[323 | 6624.06] loss=2.89 avg=2.96\n",
            "[324 | 6643.44] loss=2.72 avg=2.96\n",
            "[325 | 6665.42] loss=3.21 avg=2.96\n",
            "[326 | 6686.89] loss=2.74 avg=2.96\n",
            "[327 | 6706.24] loss=2.82 avg=2.96\n",
            "[328 | 6727.80] loss=3.06 avg=2.96\n",
            "[329 | 6747.04] loss=3.01 avg=2.96\n",
            "[330 | 6766.04] loss=2.94 avg=2.96\n",
            "[331 | 6787.00] loss=3.34 avg=2.96\n",
            "[332 | 6805.89] loss=2.19 avg=2.95\n",
            "[333 | 6824.91] loss=3.63 avg=2.96\n",
            "[334 | 6845.08] loss=2.57 avg=2.96\n",
            "[335 | 6864.85] loss=2.88 avg=2.96\n",
            "[336 | 6883.62] loss=1.59 avg=2.94\n",
            "[337 | 6902.45] loss=2.98 avg=2.94\n",
            "[338 | 6923.28] loss=2.80 avg=2.94\n",
            "[339 | 6944.08] loss=3.00 avg=2.94\n",
            "[340 | 6962.71] loss=2.71 avg=2.94\n",
            "[341 | 6983.47] loss=3.21 avg=2.94\n",
            "[342 | 7002.06] loss=3.48 avg=2.95\n",
            "[343 | 7020.82] loss=2.73 avg=2.95\n",
            "[344 | 7041.73] loss=3.35 avg=2.95\n",
            "[345 | 7060.50] loss=2.86 avg=2.95\n",
            "[346 | 7079.18] loss=3.11 avg=2.95\n",
            "[347 | 7098.99] loss=2.72 avg=2.95\n",
            "[348 | 7118.65] loss=2.93 avg=2.95\n",
            "[349 | 7137.61] loss=2.91 avg=2.95\n",
            "[350 | 7156.48] loss=2.91 avg=2.95\n",
            "[351 | 7177.26] loss=2.75 avg=2.94\n",
            "[352 | 7197.94] loss=3.01 avg=2.95\n",
            "[353 | 7216.54] loss=2.73 avg=2.94\n",
            "[354 | 7237.44] loss=1.51 avg=2.93\n",
            "[355 | 7256.19] loss=2.94 avg=2.93\n",
            "[356 | 7275.05] loss=3.20 avg=2.93\n",
            "[357 | 7295.91] loss=3.35 avg=2.94\n",
            "[358 | 7314.76] loss=2.91 avg=2.94\n",
            "[359 | 7333.48] loss=3.14 avg=2.94\n",
            "[360 | 7353.62] loss=2.84 avg=2.94\n",
            "[361 | 7372.76] loss=3.08 avg=2.94\n",
            "[362 | 7391.60] loss=2.80 avg=2.94\n",
            "[363 | 7410.46] loss=2.77 avg=2.93\n",
            "[364 | 7430.84] loss=2.70 avg=2.93\n",
            "[365 | 7451.60] loss=3.03 avg=2.93\n",
            "[366 | 7470.46] loss=2.67 avg=2.93\n",
            "[367 | 7491.05] loss=2.86 avg=2.93\n",
            "[368 | 7509.64] loss=3.14 avg=2.93\n",
            "[369 | 7528.33] loss=2.91 avg=2.93\n",
            "[370 | 7549.29] loss=3.32 avg=2.94\n",
            "[371 | 7568.01] loss=3.14 avg=2.94\n",
            "[372 | 7586.53] loss=2.78 avg=2.94\n",
            "[373 | 7607.15] loss=3.19 avg=2.94\n",
            "[374 | 7626.25] loss=2.81 avg=2.94\n",
            "[375 | 7645.25] loss=2.99 avg=2.94\n",
            "[376 | 7665.71] loss=2.76 avg=2.94\n",
            "[377 | 7686.13] loss=2.82 avg=2.93\n",
            "[378 | 7706.66] loss=2.77 avg=2.93\n",
            "[379 | 7727.88] loss=3.01 avg=2.93\n",
            "[380 | 7747.41] loss=2.84 avg=2.93\n",
            "[381 | 7766.92] loss=3.00 avg=2.93\n",
            "[382 | 7786.03] loss=2.24 avg=2.93\n",
            "[383 | 7807.76] loss=2.68 avg=2.92\n",
            "[384 | 7827.18] loss=2.89 avg=2.92\n",
            "[385 | 7846.04] loss=3.05 avg=2.93\n",
            "[386 | 7867.29] loss=3.39 avg=2.93\n",
            "[387 | 7887.08] loss=2.21 avg=2.92\n",
            "[388 | 7906.71] loss=2.70 avg=2.92\n",
            "[389 | 7928.07] loss=3.18 avg=2.92\n",
            "[390 | 7949.35] loss=2.95 avg=2.92\n",
            "[391 | 7968.85] loss=3.17 avg=2.93\n",
            "[392 | 7990.36] loss=2.66 avg=2.92\n",
            "[393 | 8009.76] loss=2.67 avg=2.92\n",
            "[394 | 8029.05] loss=3.10 avg=2.92\n",
            "[395 | 8050.35] loss=3.01 avg=2.92\n",
            "[396 | 8069.57] loss=2.84 avg=2.92\n",
            "[397 | 8088.57] loss=2.84 avg=2.92\n",
            "[398 | 8109.95] loss=2.71 avg=2.92\n",
            "[399 | 8129.08] loss=2.45 avg=2.91\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "in\n",
            "the world, and the world is not a thing, but a thing\n",
            "which is, and which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing\n",
            "\n",
            "[400 | 8241.02] loss=3.00 avg=2.92\n",
            "[401 | 8260.18] loss=2.85 avg=2.91\n",
            "[402 | 8279.39] loss=2.99 avg=2.92\n",
            "[403 | 8298.65] loss=2.82 avg=2.91\n",
            "[404 | 8317.88] loss=2.98 avg=2.92\n",
            "[405 | 8336.80] loss=2.93 avg=2.92\n",
            "[406 | 8355.82] loss=2.97 avg=2.92\n",
            "[407 | 8374.87] loss=2.80 avg=2.91\n",
            "[408 | 8394.08] loss=3.16 avg=2.92\n",
            "[409 | 8412.91] loss=3.24 avg=2.92\n",
            "[410 | 8433.79] loss=2.87 avg=2.92\n",
            "[411 | 8452.84] loss=2.87 avg=2.92\n",
            "[412 | 8471.80] loss=2.78 avg=2.92\n",
            "[413 | 8490.76] loss=3.06 avg=2.92\n",
            "[414 | 8509.74] loss=2.44 avg=2.91\n",
            "[415 | 8528.94] loss=3.04 avg=2.92\n",
            "[416 | 8548.16] loss=3.05 avg=2.92\n",
            "[417 | 8567.12] loss=3.01 avg=2.92\n",
            "[418 | 8586.04] loss=2.13 avg=2.91\n",
            "[419 | 8604.91] loss=2.89 avg=2.91\n",
            "[420 | 8623.84] loss=2.72 avg=2.91\n",
            "[421 | 8642.79] loss=2.72 avg=2.91\n",
            "[422 | 8663.74] loss=3.06 avg=2.91\n",
            "[423 | 8682.64] loss=2.88 avg=2.91\n",
            "[424 | 8701.61] loss=3.07 avg=2.91\n",
            "[425 | 8720.33] loss=3.24 avg=2.91\n",
            "[426 | 8738.99] loss=2.81 avg=2.91\n",
            "[427 | 8757.60] loss=2.89 avg=2.91\n",
            "[428 | 8776.29] loss=3.01 avg=2.91\n",
            "[429 | 8794.76] loss=3.12 avg=2.91\n",
            "[430 | 8813.37] loss=2.88 avg=2.91\n",
            "[431 | 8832.11] loss=3.26 avg=2.92\n",
            "[432 | 8850.91] loss=2.64 avg=2.91\n",
            "[433 | 8869.34] loss=2.93 avg=2.91\n",
            "[434 | 8887.64] loss=2.88 avg=2.91\n",
            "[435 | 8908.26] loss=2.87 avg=2.91\n",
            "[436 | 8927.01] loss=2.60 avg=2.91\n",
            "[437 | 8946.25] loss=2.79 avg=2.91\n",
            "[438 | 8965.09] loss=2.72 avg=2.91\n",
            "[439 | 8983.96] loss=3.03 avg=2.91\n",
            "[440 | 9003.20] loss=3.11 avg=2.91\n",
            "[441 | 9022.23] loss=2.05 avg=2.90\n",
            "[442 | 9040.79] loss=3.39 avg=2.91\n",
            "[443 | 9059.31] loss=1.98 avg=2.90\n",
            "[444 | 9077.86] loss=2.76 avg=2.90\n",
            "[445 | 9096.91] loss=2.90 avg=2.90\n",
            "[446 | 9115.89] loss=2.78 avg=2.90\n",
            "[447 | 9134.74] loss=2.80 avg=2.89\n",
            "[448 | 9156.19] loss=3.09 avg=2.90\n",
            "[449 | 9174.98] loss=3.00 avg=2.90\n",
            "[450 | 9193.74] loss=2.50 avg=2.89\n",
            "[451 | 9212.17] loss=2.75 avg=2.89\n",
            "[452 | 9230.77] loss=3.26 avg=2.90\n",
            "[453 | 9249.52] loss=2.58 avg=2.89\n",
            "[454 | 9268.04] loss=2.98 avg=2.89\n",
            "[455 | 9286.68] loss=2.74 avg=2.89\n",
            "[456 | 9305.38] loss=2.20 avg=2.89\n",
            "[457 | 9324.05] loss=2.86 avg=2.88\n",
            "[458 | 9342.62] loss=2.95 avg=2.89\n",
            "[459 | 9361.22] loss=2.94 avg=2.89\n",
            "[460 | 9379.65] loss=2.93 avg=2.89\n",
            "[461 | 9400.23] loss=2.89 avg=2.89\n",
            "[462 | 9418.96] loss=2.70 avg=2.88\n",
            "[463 | 9437.28] loss=2.70 avg=2.88\n",
            "[464 | 9455.72] loss=2.72 avg=2.88\n",
            "[465 | 9474.27] loss=2.65 avg=2.88\n",
            "[466 | 9492.83] loss=2.91 avg=2.88\n",
            "[467 | 9511.76] loss=2.80 avg=2.88\n",
            "[468 | 9530.36] loss=3.08 avg=2.88\n",
            "[469 | 9548.87] loss=2.80 avg=2.88\n",
            "[470 | 9567.32] loss=2.81 avg=2.88\n",
            "[471 | 9585.72] loss=2.83 avg=2.88\n",
            "[472 | 9604.62] loss=3.31 avg=2.88\n",
            "[473 | 9624.53] loss=2.90 avg=2.88\n",
            "[474 | 9643.80] loss=1.95 avg=2.87\n",
            "[475 | 9662.44] loss=3.30 avg=2.88\n",
            "[476 | 9680.99] loss=3.03 avg=2.88\n",
            "[477 | 9699.53] loss=3.02 avg=2.88\n",
            "[478 | 9718.11] loss=2.67 avg=2.88\n",
            "[479 | 9736.86] loss=2.68 avg=2.88\n",
            "[480 | 9755.65] loss=2.99 avg=2.88\n",
            "[481 | 9774.35] loss=2.74 avg=2.88\n",
            "[482 | 9793.08] loss=3.22 avg=2.88\n",
            "[483 | 9811.76] loss=2.84 avg=2.88\n",
            "[484 | 9830.43] loss=2.97 avg=2.88\n",
            "[485 | 9849.59] loss=3.06 avg=2.88\n",
            "[486 | 9870.40] loss=2.42 avg=2.88\n",
            "[487 | 9888.88] loss=2.81 avg=2.88\n",
            "[488 | 9907.89] loss=2.76 avg=2.88\n",
            "[489 | 9926.84] loss=2.91 avg=2.88\n",
            "[490 | 9945.71] loss=2.67 avg=2.87\n",
            "[491 | 9964.31] loss=3.09 avg=2.88\n",
            "[492 | 9983.04] loss=2.79 avg=2.88\n",
            "[493 | 10001.77] loss=2.96 avg=2.88\n",
            "[494 | 10020.42] loss=2.66 avg=2.87\n",
            "[495 | 10039.02] loss=2.71 avg=2.87\n",
            "[496 | 10058.15] loss=2.69 avg=2.87\n",
            "[497 | 10076.98] loss=2.77 avg=2.87\n",
            "[498 | 10095.93] loss=2.59 avg=2.87\n",
            "[499 | 10116.31] loss=2.26 avg=2.86\n",
            "Saving checkpoint/run1/model-500\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "),\n",
            "and the latter is the only one which can be\n",
            "conceived as a necessary condition of the existence of the\n",
            "existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is\n",
            "a necessary condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the\n",
            "\n",
            "[500 | 10225.27] loss=2.77 avg=2.86\n",
            "[501 | 10244.34] loss=2.90 avg=2.86\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 10: Creating a Training Model directory\n",
        "\n",
        "#Creating a Training Model directory named 'tgmodel'\n",
        "\n",
        "import os\n",
        "run_dir = '/content/gpt-2/models/tgmodel'\n",
        "if not os.path.exists(run_dir):\n",
        "  os.makedirs(run_dir)\n",
        "  "
      ],
      "metadata": {
        "id": "ynUNLAsMZdeK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 10A: Copying training Files\n",
        "\n",
        "!cp /content/gpt-2/src/checkpoint/run1/model-500.data-00000-of-00001 /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/src/checkpoint/run1/checkpoint /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/src/checkpoint/run1/model-500.index /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/src/checkpoint/run1/model-500.meta /content/gpt-2/models/tgmodel"
      ],
      "metadata": {
        "id": "XetEMUrmaJpn"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}