{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1BIqNacyPdCrVpEypXyWd0C7hvtAWhC3e",
      "authorship_tag": "ABX9TyMVBNgY5719z0yPNMhWPyUq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bforoura/Transformers/blob/main/nlp_transformers_ch6_ex3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxS0FWEyFDGu"
      },
      "outputs": [],
      "source": [
        "#@title Step 1: Activate GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 2: Cloning the OpenAI GPT-2 Repository\n",
        "\n",
        "\n",
        "!git clone https://github.com/openai/gpt-2.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh06O2FsKxQg",
        "outputId": "89d70582-9a33-4424-f3fc-30c9915566cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 233, done.\u001b[K\n",
            "remote: Total 233 (delta 0), reused 0 (delta 0), pack-reused 233\u001b[K\n",
            "Receiving objects: 100% (233/233), 4.38 MiB | 15.46 MiB/s, done.\n",
            "Resolving deltas: 100% (124/124), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 3: Installing the requirements\n",
        "\n",
        "\n",
        "# import os          # when the VM restarts import os necessary\n",
        "# os.chdir(\"/content/gpt-2\")\n",
        "# !pip3 install -r requirements.txt\n",
        "\n",
        "# The requirements have been installed automatically.\n",
        "\n",
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.15.2\n",
        "\n",
        "#This notebook requires toposort, which is a topological sort algorithm:\n",
        "!pip install toposort\n"
      ],
      "metadata": {
        "id": "iZsjFx-LLTYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 4: Checking the Version of TensorFlow\n",
        "\n",
        "#Colab includes Tensorflow 2.x by default but we need a lower version\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtIklTBFMWzD",
        "outputId": "50701b23-fb7f-4ea7-f166-f89481d960d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 5: Downloading the 117M parameter GPT-2 Model\n",
        "\n",
        "# run code and send argument\n",
        "import os \n",
        "\n",
        "os.chdir(\"/content/gpt-2\")\n",
        "!python3 download_model.py '117M'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUPX95ZTMxg5",
        "outputId": "cb6ddcec-f681-47c3-c47d-64e3353661d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 1.05Mit/s]                                                     \n",
            "Fetching encoder.json: 1.04Mit [00:00, 5.51Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.06Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:11, 42.2Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 5.42Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 2.93Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 3.24Mit/s]                                                       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 6: Copying the Project Resources to src\n",
        "\n",
        "!cp /content/dset.txt /content/gpt-2/src/\n",
        "!cp -r /content/gpt-2/models/ /content/gpt-2/src/\n"
      ],
      "metadata": {
        "id": "N4Fw-c5XNpnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 7: Copying the N Shepperd Training Files\n",
        "#Referfence GitHub repository: https://github.com/nshepperd/gpt-2\n",
        "\n",
        "import os # import after runtime is restarted\n",
        "\n",
        "!cp /content/train.py /content/gpt-2/src/\n",
        "!cp /content/load_dataset.py /content/gpt-2/src/\n",
        "!cp /content/encode.py /content/gpt-2/src/\n",
        "!cp /content/accumulate.py /content/gpt-2/src/\n",
        "!cp /content/memory_saving_gradients.py /content/gpt-2/src/\n"
      ],
      "metadata": {
        "id": "gGiTzqOuOeic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 8: Encoding dataset\n",
        "\n",
        "import os # import after runtime is restarted\n",
        "\n",
        "os.chdir(\"/content/gpt-2/src/\")\n",
        "model_name=\"117M\"\n",
        "!python /content/gpt-2/src/encode.py dset.txt out.npz \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU2mwKriPmcO",
        "outputId": "60338cd2-3b7e-4be5-fbd3-865fcd49c360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading files\n",
            "100% 1/1 [00:09<00:00,  9.12s/it]\n",
            "Writing out.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 9: Training the Model\n",
        "\n",
        "#Model saved after 1000 steps\n",
        "import os # import after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2/src/\")\n",
        "\n",
        "!python train.py --dataset out.npz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lST6tk4hR27y",
        "outputId": "370ab1dc-772b-40dd-f72b-bde60bd5a883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:89: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:92: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2022-11-07 15:49:55.175411: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-11-07 15:49:55.178555: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
            "2022-11-07 15:49:55.178772: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x17759c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-11-07 15:49:55.178805: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "WARNING:tensorflow:From train.py:93: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From train.py:118: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:122: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:145: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:148: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:150: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:153: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:157: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "Loading checkpoint models/117M/model.ckpt\n",
            "Loading dataset...\n",
            "100% 1/1 [00:00<00:00, 11.94it/s]\n",
            "dataset has 2548152 tokens\n",
            "Training...\n",
            "[1 | 22.35] loss=3.44 avg=3.44\n",
            "[2 | 41.52] loss=2.99 avg=3.21\n",
            "[3 | 60.72] loss=3.30 avg=3.24\n",
            "[4 | 79.90] loss=3.20 avg=3.23\n",
            "[5 | 99.07] loss=3.59 avg=3.30\n",
            "[6 | 118.27] loss=3.39 avg=3.32\n",
            "[7 | 137.47] loss=3.30 avg=3.32\n",
            "[8 | 156.63] loss=3.57 avg=3.35\n",
            "[9 | 175.91] loss=2.77 avg=3.28\n",
            "[10 | 195.62] loss=3.08 avg=3.26\n",
            "[11 | 215.17] loss=3.11 avg=3.25\n",
            "[12 | 234.54] loss=3.00 avg=3.23\n",
            "[13 | 254.00] loss=3.37 avg=3.24\n",
            "[14 | 273.53] loss=3.16 avg=3.23\n",
            "[15 | 293.08] loss=3.22 avg=3.23\n",
            "[16 | 312.79] loss=3.12 avg=3.22\n",
            "[17 | 332.26] loss=3.16 avg=3.22\n",
            "[18 | 351.80] loss=3.09 avg=3.21\n",
            "[19 | 371.33] loss=3.24 avg=3.21\n",
            "[20 | 390.47] loss=3.24 avg=3.21\n",
            "[21 | 409.90] loss=3.27 avg=3.22\n",
            "[22 | 429.12] loss=3.03 avg=3.21\n",
            "[23 | 448.51] loss=3.22 avg=3.21\n",
            "[24 | 467.73] loss=3.53 avg=3.22\n",
            "[25 | 487.12] loss=3.01 avg=3.21\n",
            "[26 | 506.79] loss=3.31 avg=3.22\n",
            "[27 | 526.40] loss=3.25 avg=3.22\n",
            "[28 | 546.16] loss=3.22 avg=3.22\n",
            "[29 | 565.88] loss=3.12 avg=3.22\n",
            "[30 | 585.87] loss=3.33 avg=3.22\n",
            "[31 | 605.91] loss=3.02 avg=3.21\n",
            "[32 | 625.49] loss=3.21 avg=3.21\n",
            "[33 | 645.22] loss=3.03 avg=3.21\n",
            "[34 | 664.46] loss=3.09 avg=3.20\n",
            "[35 | 683.89] loss=2.95 avg=3.19\n",
            "[36 | 703.47] loss=3.05 avg=3.19\n",
            "[37 | 722.52] loss=3.35 avg=3.19\n",
            "[38 | 741.33] loss=3.26 avg=3.20\n",
            "[39 | 760.29] loss=3.01 avg=3.19\n",
            "[40 | 779.18] loss=3.19 avg=3.19\n",
            "[41 | 797.84] loss=2.61 avg=3.17\n",
            "[42 | 816.64] loss=3.29 avg=3.18\n",
            "[43 | 835.23] loss=2.52 avg=3.16\n",
            "[44 | 853.96] loss=3.10 avg=3.16\n",
            "[45 | 872.63] loss=3.28 avg=3.16\n",
            "[46 | 891.39] loss=3.18 avg=3.16\n",
            "[47 | 910.28] loss=3.19 avg=3.16\n",
            "[48 | 929.09] loss=3.06 avg=3.16\n",
            "[49 | 948.01] loss=3.17 avg=3.16\n",
            "[50 | 966.82] loss=3.27 avg=3.16\n",
            "[51 | 985.52] loss=3.09 avg=3.16\n",
            "[52 | 1004.52] loss=2.92 avg=3.15\n",
            "[53 | 1023.31] loss=3.26 avg=3.16\n",
            "[54 | 1042.10] loss=1.64 avg=3.12\n",
            "[55 | 1060.79] loss=2.88 avg=3.11\n",
            "[56 | 1079.44] loss=2.93 avg=3.11\n",
            "[57 | 1098.13] loss=3.27 avg=3.11\n",
            "[58 | 1116.77] loss=3.41 avg=3.12\n",
            "[59 | 1135.58] loss=3.04 avg=3.12\n",
            "[60 | 1154.35] loss=3.29 avg=3.12\n",
            "[61 | 1173.09] loss=2.94 avg=3.12\n",
            "[62 | 1191.91] loss=3.14 avg=3.12\n",
            "[63 | 1210.42] loss=3.27 avg=3.12\n",
            "[64 | 1229.06] loss=3.39 avg=3.13\n",
            "[65 | 1247.87] loss=3.37 avg=3.13\n",
            "[66 | 1266.55] loss=1.87 avg=3.11\n",
            "[67 | 1285.04] loss=2.54 avg=3.10\n",
            "[68 | 1303.72] loss=2.86 avg=3.09\n",
            "[69 | 1322.48] loss=3.08 avg=3.09\n",
            "[70 | 1341.39] loss=3.16 avg=3.09\n",
            "[71 | 1360.21] loss=2.75 avg=3.09\n",
            "[72 | 1379.50] loss=3.00 avg=3.08\n",
            "[73 | 1398.72] loss=3.46 avg=3.09\n",
            "[74 | 1418.45] loss=3.29 avg=3.09\n",
            "[75 | 1437.83] loss=2.98 avg=3.09\n",
            "[76 | 1456.83] loss=3.24 avg=3.10\n",
            "[77 | 1475.85] loss=3.08 avg=3.09\n",
            "[78 | 1495.38] loss=3.09 avg=3.09\n",
            "[79 | 1515.31] loss=3.13 avg=3.10\n",
            "[80 | 1535.39] loss=2.90 avg=3.09\n",
            "[81 | 1555.48] loss=3.02 avg=3.09\n",
            "[82 | 1575.00] loss=2.98 avg=3.09\n",
            "[83 | 1594.69] loss=2.94 avg=3.09\n",
            "[84 | 1614.66] loss=3.09 avg=3.09\n",
            "[85 | 1634.60] loss=2.78 avg=3.08\n",
            "[86 | 1654.03] loss=3.07 avg=3.08\n",
            "[87 | 1673.53] loss=2.91 avg=3.08\n",
            "[88 | 1692.96] loss=2.76 avg=3.07\n",
            "[89 | 1712.08] loss=3.63 avg=3.08\n",
            "[90 | 1731.41] loss=3.02 avg=3.08\n",
            "[91 | 1750.41] loss=2.86 avg=3.08\n",
            "[92 | 1769.80] loss=2.70 avg=3.07\n",
            "[93 | 1789.29] loss=3.24 avg=3.07\n",
            "[94 | 1808.79] loss=3.12 avg=3.07\n",
            "[95 | 1828.26] loss=3.50 avg=3.08\n",
            "[96 | 1847.73] loss=2.95 avg=3.08\n",
            "[97 | 1867.65] loss=2.88 avg=3.08\n",
            "[98 | 1887.66] loss=3.20 avg=3.08\n",
            "[99 | 1907.37] loss=3.00 avg=3.08\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"The\n",
            "\n",
            "\"\n",
            "\n",
            "[100 | 2020.67] loss=2.88 avg=3.07\n",
            "[101 | 2040.18] loss=3.20 avg=3.08\n",
            "[102 | 2059.81] loss=2.86 avg=3.07\n",
            "[103 | 2079.25] loss=2.95 avg=3.07\n",
            "[104 | 2098.73] loss=3.00 avg=3.07\n",
            "[105 | 2118.23] loss=3.49 avg=3.08\n",
            "[106 | 2137.64] loss=3.15 avg=3.08\n",
            "[107 | 2157.07] loss=3.11 avg=3.08\n",
            "[108 | 2176.21] loss=2.93 avg=3.07\n",
            "[109 | 2195.28] loss=3.13 avg=3.08\n",
            "[110 | 2214.65] loss=3.07 avg=3.08\n",
            "[111 | 2233.92] loss=3.17 avg=3.08\n",
            "[112 | 2253.04] loss=2.76 avg=3.07\n",
            "[113 | 2272.04] loss=3.32 avg=3.08\n",
            "[114 | 2291.09] loss=3.21 avg=3.08\n",
            "[115 | 2310.37] loss=3.19 avg=3.08\n",
            "[116 | 2329.60] loss=2.90 avg=3.08\n",
            "[117 | 2348.91] loss=3.04 avg=3.08\n",
            "[118 | 2367.93] loss=2.98 avg=3.08\n",
            "[119 | 2387.29] loss=3.02 avg=3.07\n",
            "[120 | 2406.44] loss=3.04 avg=3.07\n",
            "[121 | 2425.48] loss=2.98 avg=3.07\n",
            "[122 | 2444.52] loss=2.95 avg=3.07\n",
            "[123 | 2463.73] loss=3.05 avg=3.07\n",
            "[124 | 2482.80] loss=3.03 avg=3.07\n",
            "[125 | 2501.68] loss=3.04 avg=3.07\n",
            "[126 | 2520.68] loss=3.45 avg=3.07\n",
            "[127 | 2539.87] loss=3.34 avg=3.08\n",
            "[128 | 2558.88] loss=3.06 avg=3.08\n",
            "[129 | 2577.88] loss=3.28 avg=3.08\n",
            "[130 | 2596.63] loss=2.29 avg=3.07\n",
            "[131 | 2615.35] loss=3.23 avg=3.07\n",
            "[132 | 2633.92] loss=3.32 avg=3.08\n",
            "[133 | 2652.80] loss=3.06 avg=3.08\n",
            "[134 | 2672.66] loss=2.82 avg=3.07\n",
            "[135 | 2692.51] loss=2.81 avg=3.07\n",
            "[136 | 2712.69] loss=3.05 avg=3.07\n",
            "[137 | 2732.24] loss=2.98 avg=3.07\n",
            "[138 | 2752.49] loss=3.29 avg=3.07\n",
            "[139 | 2772.66] loss=3.03 avg=3.07\n",
            "[140 | 2792.12] loss=2.89 avg=3.07\n",
            "[141 | 2812.61] loss=2.81 avg=3.06\n",
            "[142 | 2832.45] loss=3.03 avg=3.06\n",
            "[143 | 2853.16] loss=3.08 avg=3.06\n",
            "[144 | 2873.66] loss=3.30 avg=3.07\n",
            "[145 | 2893.92] loss=3.04 avg=3.07\n",
            "[146 | 2913.91] loss=3.10 avg=3.07\n",
            "[147 | 2933.31] loss=2.98 avg=3.07\n",
            "[148 | 2952.45] loss=3.00 avg=3.06\n",
            "[149 | 2971.95] loss=3.08 avg=3.06\n",
            "[150 | 2991.34] loss=3.17 avg=3.07\n",
            "[151 | 3010.98] loss=3.07 avg=3.07\n",
            "[152 | 3030.37] loss=3.06 avg=3.07\n",
            "[153 | 3049.59] loss=2.78 avg=3.06\n",
            "[154 | 3068.63] loss=3.09 avg=3.06\n",
            "[155 | 3087.88] loss=3.05 avg=3.06\n",
            "[156 | 3107.06] loss=2.86 avg=3.06\n",
            "[157 | 3126.39] loss=2.46 avg=3.05\n",
            "[158 | 3145.57] loss=3.41 avg=3.06\n",
            "[159 | 3164.60] loss=3.19 avg=3.06\n",
            "[160 | 3183.74] loss=2.95 avg=3.06\n",
            "[161 | 3202.95] loss=3.31 avg=3.06\n",
            "[162 | 3222.00] loss=2.84 avg=3.06\n",
            "[163 | 3241.35] loss=2.97 avg=3.06\n",
            "[164 | 3260.34] loss=2.94 avg=3.06\n",
            "[165 | 3279.46] loss=2.88 avg=3.05\n",
            "[166 | 3298.69] loss=3.14 avg=3.05\n",
            "[167 | 3317.77] loss=2.68 avg=3.05\n",
            "[168 | 3337.01] loss=2.77 avg=3.05\n",
            "[169 | 3358.62] loss=2.18 avg=3.04\n",
            "[170 | 3377.73] loss=3.24 avg=3.04\n",
            "[171 | 3396.82] loss=2.79 avg=3.03\n",
            "[172 | 3418.36] loss=2.87 avg=3.03\n",
            "[173 | 3437.63] loss=2.97 avg=3.03\n",
            "[174 | 3456.96] loss=2.86 avg=3.03\n",
            "[175 | 3476.25] loss=2.85 avg=3.03\n",
            "[176 | 3497.40] loss=2.92 avg=3.03\n",
            "[177 | 3516.65] loss=2.64 avg=3.02\n",
            "[178 | 3535.77] loss=2.97 avg=3.02\n",
            "[179 | 3557.48] loss=2.97 avg=3.02\n",
            "[180 | 3576.71] loss=3.08 avg=3.02\n",
            "[181 | 3595.90] loss=2.80 avg=3.02\n",
            "[182 | 3617.36] loss=3.02 avg=3.02\n",
            "[183 | 3636.63] loss=3.21 avg=3.02\n",
            "[184 | 3655.81] loss=3.04 avg=3.02\n",
            "[185 | 3675.60] loss=0.84 avg=3.00\n",
            "[186 | 3696.32] loss=2.76 avg=2.99\n",
            "[187 | 3715.37] loss=2.86 avg=2.99\n",
            "[188 | 3734.52] loss=3.00 avg=2.99\n",
            "[189 | 3753.81] loss=2.86 avg=2.99\n",
            "[190 | 3774.92] loss=2.83 avg=2.99\n",
            "[191 | 3794.14] loss=2.94 avg=2.99\n",
            "[192 | 3813.35] loss=3.05 avg=2.99\n",
            "[193 | 3834.32] loss=2.95 avg=2.99\n",
            "[194 | 3853.71] loss=3.20 avg=2.99\n",
            "[195 | 3872.94] loss=2.97 avg=2.99\n",
            "[196 | 3891.82] loss=2.82 avg=2.99\n",
            "[197 | 3912.88] loss=3.28 avg=2.99\n",
            "[198 | 3932.14] loss=3.15 avg=2.99\n",
            "[199 | 3951.12] loss=3.08 avg=2.99\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "proof of the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and the existence of a God, and\n",
            "\n",
            "[200 | 4060.81] loss=2.84 avg=2.99\n",
            "[201 | 4082.10] loss=3.12 avg=2.99\n",
            "[202 | 4101.09] loss=2.80 avg=2.99\n",
            "[203 | 4120.32] loss=3.11 avg=2.99\n",
            "[204 | 4141.50] loss=3.19 avg=3.00\n",
            "[205 | 4160.71] loss=3.20 avg=3.00\n",
            "[206 | 4179.95] loss=3.35 avg=3.00\n",
            "[207 | 4201.12] loss=2.97 avg=3.00\n",
            "[208 | 4220.31] loss=3.02 avg=3.00\n",
            "[209 | 4239.60] loss=3.02 avg=3.00\n",
            "[210 | 4260.01] loss=3.10 avg=3.00\n",
            "[211 | 4280.08] loss=2.92 avg=3.00\n",
            "[212 | 4299.17] loss=1.31 avg=2.98\n",
            "[213 | 4318.37] loss=2.25 avg=2.97\n",
            "[214 | 4339.61] loss=3.26 avg=2.98\n",
            "[215 | 4358.81] loss=2.99 avg=2.98\n",
            "[216 | 4378.05] loss=2.93 avg=2.98\n",
            "[217 | 4399.61] loss=3.13 avg=2.98\n",
            "[218 | 4418.89] loss=2.88 avg=2.98\n",
            "[219 | 4437.98] loss=3.12 avg=2.98\n",
            "[220 | 4459.10] loss=2.95 avg=2.98\n",
            "[221 | 4478.35] loss=3.07 avg=2.98\n",
            "[222 | 4497.68] loss=3.17 avg=2.98\n",
            "[223 | 4517.12] loss=2.66 avg=2.98\n",
            "[224 | 4538.26] loss=3.47 avg=2.98\n",
            "[225 | 4557.42] loss=2.91 avg=2.98\n",
            "[226 | 4576.57] loss=3.12 avg=2.98\n",
            "[227 | 4597.64] loss=3.54 avg=2.99\n",
            "[228 | 4616.91] loss=2.40 avg=2.98\n",
            "[229 | 4636.16] loss=3.03 avg=2.98\n",
            "[230 | 4657.59] loss=2.86 avg=2.98\n",
            "[231 | 4677.02] loss=3.17 avg=2.99\n",
            "[232 | 4696.25] loss=2.99 avg=2.99\n",
            "[233 | 4717.40] loss=2.41 avg=2.98\n",
            "[234 | 4736.53] loss=3.26 avg=2.98\n",
            "[235 | 4756.02] loss=3.15 avg=2.98\n",
            "[236 | 4775.94] loss=2.78 avg=2.98\n",
            "[237 | 4796.02] loss=3.02 avg=2.98\n",
            "[238 | 4815.15] loss=3.37 avg=2.99\n",
            "[239 | 4834.27] loss=2.87 avg=2.99\n",
            "[240 | 4855.42] loss=3.02 avg=2.99\n",
            "[241 | 4874.62] loss=2.90 avg=2.98\n",
            "[242 | 4894.00] loss=3.09 avg=2.99\n",
            "[243 | 4915.62] loss=3.16 avg=2.99\n",
            "[244 | 4935.37] loss=3.07 avg=2.99\n",
            "[245 | 4954.88] loss=2.88 avg=2.99\n",
            "[246 | 4976.74] loss=2.93 avg=2.99\n",
            "[247 | 4996.15] loss=3.18 avg=2.99\n",
            "[248 | 5015.52] loss=3.12 avg=2.99\n",
            "[249 | 5036.72] loss=3.10 avg=2.99\n",
            "[250 | 5056.10] loss=2.99 avg=2.99\n",
            "[251 | 5075.60] loss=3.13 avg=2.99\n",
            "[252 | 5094.91] loss=2.86 avg=2.99\n",
            "[253 | 5116.50] loss=2.79 avg=2.99\n",
            "[254 | 5136.08] loss=2.65 avg=2.99\n",
            "[255 | 5155.59] loss=2.77 avg=2.98\n",
            "[256 | 5174.84] loss=3.09 avg=2.98\n",
            "[257 | 5196.15] loss=2.76 avg=2.98\n",
            "[258 | 5215.68] loss=3.38 avg=2.99\n",
            "[259 | 5234.96] loss=3.21 avg=2.99\n",
            "[260 | 5256.48] loss=2.96 avg=2.99\n",
            "[261 | 5276.04] loss=2.78 avg=2.99\n",
            "[262 | 5295.67] loss=2.77 avg=2.98\n",
            "[263 | 5317.08] loss=3.32 avg=2.99\n",
            "[264 | 5336.53] loss=2.70 avg=2.98\n",
            "[265 | 5355.94] loss=3.01 avg=2.98\n",
            "[266 | 5377.47] loss=3.06 avg=2.99\n",
            "[267 | 5397.08] loss=3.04 avg=2.99\n",
            "[268 | 5416.92] loss=2.83 avg=2.98\n",
            "[269 | 5438.37] loss=3.15 avg=2.99\n",
            "[270 | 5457.89] loss=3.18 avg=2.99\n",
            "[271 | 5477.39] loss=2.73 avg=2.99\n",
            "[272 | 5496.84] loss=2.81 avg=2.98\n",
            "[273 | 5518.31] loss=2.79 avg=2.98\n",
            "[274 | 5537.58] loss=3.23 avg=2.98\n",
            "[275 | 5556.91] loss=2.43 avg=2.98\n",
            "[276 | 5578.07] loss=3.08 avg=2.98\n",
            "[277 | 5597.66] loss=3.38 avg=2.98\n",
            "[278 | 5617.16] loss=2.62 avg=2.98\n",
            "[279 | 5638.91] loss=2.91 avg=2.98\n",
            "[280 | 5658.34] loss=2.68 avg=2.98\n",
            "[281 | 5679.86] loss=3.14 avg=2.98\n",
            "[282 | 5701.70] loss=2.85 avg=2.98\n",
            "[283 | 5721.18] loss=3.19 avg=2.98\n",
            "[284 | 5740.62] loss=2.86 avg=2.98\n",
            "[285 | 5761.86] loss=2.85 avg=2.98\n",
            "[286 | 5781.35] loss=2.89 avg=2.98\n",
            "[287 | 5800.78] loss=2.70 avg=2.97\n",
            "[288 | 5821.37] loss=2.68 avg=2.97\n",
            "[289 | 5841.72] loss=2.82 avg=2.97\n",
            "[290 | 5861.38] loss=2.81 avg=2.97\n",
            "[291 | 5880.90] loss=3.06 avg=2.97\n",
            "[292 | 5902.44] loss=2.83 avg=2.97\n",
            "[293 | 5921.67] loss=2.93 avg=2.96\n",
            "[294 | 5943.10] loss=3.13 avg=2.97\n",
            "[295 | 5964.43] loss=2.93 avg=2.97\n",
            "[296 | 5984.04] loss=2.91 avg=2.97\n",
            "[297 | 6003.89] loss=3.40 avg=2.97\n",
            "[298 | 6025.88] loss=2.94 avg=2.97\n",
            "[299 | 6045.42] loss=2.98 avg=2.97\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " in the world, and the world is not a place of the mind, but a place of the mind, and the mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place of the mind.\n",
            "\n",
            "The mind is not a place of the mind, but a place\n",
            "\n",
            "[300 | 6157.91] loss=2.97 avg=2.97\n",
            "[301 | 6178.25] loss=3.09 avg=2.97\n",
            "[302 | 6198.71] loss=2.88 avg=2.97\n",
            "[303 | 6220.23] loss=2.82 avg=2.97\n",
            "[304 | 6239.65] loss=2.81 avg=2.97\n",
            "[305 | 6259.07] loss=2.97 avg=2.97\n",
            "[306 | 6280.84] loss=2.98 avg=2.97\n",
            "[307 | 6300.27] loss=3.25 avg=2.97\n",
            "[308 | 6319.59] loss=2.65 avg=2.97\n",
            "[309 | 6341.08] loss=2.96 avg=2.97\n",
            "[310 | 6360.71] loss=3.11 avg=2.97\n",
            "[311 | 6380.07] loss=2.86 avg=2.97\n",
            "[312 | 6401.31] loss=2.25 avg=2.96\n",
            "[313 | 6420.84] loss=2.79 avg=2.96\n",
            "[314 | 6442.54] loss=2.91 avg=2.96\n",
            "[315 | 6464.28] loss=2.85 avg=2.96\n",
            "[316 | 6483.53] loss=3.53 avg=2.96\n",
            "[317 | 6503.08] loss=2.97 avg=2.96\n",
            "[318 | 6522.92] loss=3.11 avg=2.96\n",
            "[319 | 6544.38] loss=3.16 avg=2.97\n",
            "[320 | 6563.78] loss=2.33 avg=2.96\n",
            "[321 | 6583.45] loss=3.08 avg=2.96\n",
            "[322 | 6604.68] loss=2.98 avg=2.96\n",
            "[323 | 6624.06] loss=2.89 avg=2.96\n",
            "[324 | 6643.44] loss=2.72 avg=2.96\n",
            "[325 | 6665.42] loss=3.21 avg=2.96\n",
            "[326 | 6686.89] loss=2.74 avg=2.96\n",
            "[327 | 6706.24] loss=2.82 avg=2.96\n",
            "[328 | 6727.80] loss=3.06 avg=2.96\n",
            "[329 | 6747.04] loss=3.01 avg=2.96\n",
            "[330 | 6766.04] loss=2.94 avg=2.96\n",
            "[331 | 6787.00] loss=3.34 avg=2.96\n",
            "[332 | 6805.89] loss=2.19 avg=2.95\n",
            "[333 | 6824.91] loss=3.63 avg=2.96\n",
            "[334 | 6845.08] loss=2.57 avg=2.96\n",
            "[335 | 6864.85] loss=2.88 avg=2.96\n",
            "[336 | 6883.62] loss=1.59 avg=2.94\n",
            "[337 | 6902.45] loss=2.98 avg=2.94\n",
            "[338 | 6923.28] loss=2.80 avg=2.94\n",
            "[339 | 6944.08] loss=3.00 avg=2.94\n",
            "[340 | 6962.71] loss=2.71 avg=2.94\n",
            "[341 | 6983.47] loss=3.21 avg=2.94\n",
            "[342 | 7002.06] loss=3.48 avg=2.95\n",
            "[343 | 7020.82] loss=2.73 avg=2.95\n",
            "[344 | 7041.73] loss=3.35 avg=2.95\n",
            "[345 | 7060.50] loss=2.86 avg=2.95\n",
            "[346 | 7079.18] loss=3.11 avg=2.95\n",
            "[347 | 7098.99] loss=2.72 avg=2.95\n",
            "[348 | 7118.65] loss=2.93 avg=2.95\n",
            "[349 | 7137.61] loss=2.91 avg=2.95\n",
            "[350 | 7156.48] loss=2.91 avg=2.95\n",
            "[351 | 7177.26] loss=2.75 avg=2.94\n",
            "[352 | 7197.94] loss=3.01 avg=2.95\n",
            "[353 | 7216.54] loss=2.73 avg=2.94\n",
            "[354 | 7237.44] loss=1.51 avg=2.93\n",
            "[355 | 7256.19] loss=2.94 avg=2.93\n",
            "[356 | 7275.05] loss=3.20 avg=2.93\n",
            "[357 | 7295.91] loss=3.35 avg=2.94\n",
            "[358 | 7314.76] loss=2.91 avg=2.94\n",
            "[359 | 7333.48] loss=3.14 avg=2.94\n",
            "[360 | 7353.62] loss=2.84 avg=2.94\n",
            "[361 | 7372.76] loss=3.08 avg=2.94\n",
            "[362 | 7391.60] loss=2.80 avg=2.94\n",
            "[363 | 7410.46] loss=2.77 avg=2.93\n",
            "[364 | 7430.84] loss=2.70 avg=2.93\n",
            "[365 | 7451.60] loss=3.03 avg=2.93\n",
            "[366 | 7470.46] loss=2.67 avg=2.93\n",
            "[367 | 7491.05] loss=2.86 avg=2.93\n",
            "[368 | 7509.64] loss=3.14 avg=2.93\n",
            "[369 | 7528.33] loss=2.91 avg=2.93\n",
            "[370 | 7549.29] loss=3.32 avg=2.94\n",
            "[371 | 7568.01] loss=3.14 avg=2.94\n",
            "[372 | 7586.53] loss=2.78 avg=2.94\n",
            "[373 | 7607.15] loss=3.19 avg=2.94\n",
            "[374 | 7626.25] loss=2.81 avg=2.94\n",
            "[375 | 7645.25] loss=2.99 avg=2.94\n",
            "[376 | 7665.71] loss=2.76 avg=2.94\n",
            "[377 | 7686.13] loss=2.82 avg=2.93\n",
            "[378 | 7706.66] loss=2.77 avg=2.93\n",
            "[379 | 7727.88] loss=3.01 avg=2.93\n",
            "[380 | 7747.41] loss=2.84 avg=2.93\n",
            "[381 | 7766.92] loss=3.00 avg=2.93\n",
            "[382 | 7786.03] loss=2.24 avg=2.93\n",
            "[383 | 7807.76] loss=2.68 avg=2.92\n",
            "[384 | 7827.18] loss=2.89 avg=2.92\n",
            "[385 | 7846.04] loss=3.05 avg=2.93\n",
            "[386 | 7867.29] loss=3.39 avg=2.93\n",
            "[387 | 7887.08] loss=2.21 avg=2.92\n",
            "[388 | 7906.71] loss=2.70 avg=2.92\n",
            "[389 | 7928.07] loss=3.18 avg=2.92\n",
            "[390 | 7949.35] loss=2.95 avg=2.92\n",
            "[391 | 7968.85] loss=3.17 avg=2.93\n",
            "[392 | 7990.36] loss=2.66 avg=2.92\n",
            "[393 | 8009.76] loss=2.67 avg=2.92\n",
            "[394 | 8029.05] loss=3.10 avg=2.92\n",
            "[395 | 8050.35] loss=3.01 avg=2.92\n",
            "[396 | 8069.57] loss=2.84 avg=2.92\n",
            "[397 | 8088.57] loss=2.84 avg=2.92\n",
            "[398 | 8109.95] loss=2.71 avg=2.92\n",
            "[399 | 8129.08] loss=2.45 avg=2.91\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "in\n",
            "the world, and the world is not a thing, but a thing\n",
            "which is, and which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing which is, and\n",
            "which is not, a thing.\n",
            "\n",
            "The world is not a thing, but a thing\n",
            "\n",
            "[400 | 8241.02] loss=3.00 avg=2.92\n",
            "[401 | 8260.18] loss=2.85 avg=2.91\n",
            "[402 | 8279.39] loss=2.99 avg=2.92\n",
            "[403 | 8298.65] loss=2.82 avg=2.91\n",
            "[404 | 8317.88] loss=2.98 avg=2.92\n",
            "[405 | 8336.80] loss=2.93 avg=2.92\n",
            "[406 | 8355.82] loss=2.97 avg=2.92\n",
            "[407 | 8374.87] loss=2.80 avg=2.91\n",
            "[408 | 8394.08] loss=3.16 avg=2.92\n",
            "[409 | 8412.91] loss=3.24 avg=2.92\n",
            "[410 | 8433.79] loss=2.87 avg=2.92\n",
            "[411 | 8452.84] loss=2.87 avg=2.92\n",
            "[412 | 8471.80] loss=2.78 avg=2.92\n",
            "[413 | 8490.76] loss=3.06 avg=2.92\n",
            "[414 | 8509.74] loss=2.44 avg=2.91\n",
            "[415 | 8528.94] loss=3.04 avg=2.92\n",
            "[416 | 8548.16] loss=3.05 avg=2.92\n",
            "[417 | 8567.12] loss=3.01 avg=2.92\n",
            "[418 | 8586.04] loss=2.13 avg=2.91\n",
            "[419 | 8604.91] loss=2.89 avg=2.91\n",
            "[420 | 8623.84] loss=2.72 avg=2.91\n",
            "[421 | 8642.79] loss=2.72 avg=2.91\n",
            "[422 | 8663.74] loss=3.06 avg=2.91\n",
            "[423 | 8682.64] loss=2.88 avg=2.91\n",
            "[424 | 8701.61] loss=3.07 avg=2.91\n",
            "[425 | 8720.33] loss=3.24 avg=2.91\n",
            "[426 | 8738.99] loss=2.81 avg=2.91\n",
            "[427 | 8757.60] loss=2.89 avg=2.91\n",
            "[428 | 8776.29] loss=3.01 avg=2.91\n",
            "[429 | 8794.76] loss=3.12 avg=2.91\n",
            "[430 | 8813.37] loss=2.88 avg=2.91\n",
            "[431 | 8832.11] loss=3.26 avg=2.92\n",
            "[432 | 8850.91] loss=2.64 avg=2.91\n",
            "[433 | 8869.34] loss=2.93 avg=2.91\n",
            "[434 | 8887.64] loss=2.88 avg=2.91\n",
            "[435 | 8908.26] loss=2.87 avg=2.91\n",
            "[436 | 8927.01] loss=2.60 avg=2.91\n",
            "[437 | 8946.25] loss=2.79 avg=2.91\n",
            "[438 | 8965.09] loss=2.72 avg=2.91\n",
            "[439 | 8983.96] loss=3.03 avg=2.91\n",
            "[440 | 9003.20] loss=3.11 avg=2.91\n",
            "[441 | 9022.23] loss=2.05 avg=2.90\n",
            "[442 | 9040.79] loss=3.39 avg=2.91\n",
            "[443 | 9059.31] loss=1.98 avg=2.90\n",
            "[444 | 9077.86] loss=2.76 avg=2.90\n",
            "[445 | 9096.91] loss=2.90 avg=2.90\n",
            "[446 | 9115.89] loss=2.78 avg=2.90\n",
            "[447 | 9134.74] loss=2.80 avg=2.89\n",
            "[448 | 9156.19] loss=3.09 avg=2.90\n",
            "[449 | 9174.98] loss=3.00 avg=2.90\n",
            "[450 | 9193.74] loss=2.50 avg=2.89\n",
            "[451 | 9212.17] loss=2.75 avg=2.89\n",
            "[452 | 9230.77] loss=3.26 avg=2.90\n",
            "[453 | 9249.52] loss=2.58 avg=2.89\n",
            "[454 | 9268.04] loss=2.98 avg=2.89\n",
            "[455 | 9286.68] loss=2.74 avg=2.89\n",
            "[456 | 9305.38] loss=2.20 avg=2.89\n",
            "[457 | 9324.05] loss=2.86 avg=2.88\n",
            "[458 | 9342.62] loss=2.95 avg=2.89\n",
            "[459 | 9361.22] loss=2.94 avg=2.89\n",
            "[460 | 9379.65] loss=2.93 avg=2.89\n",
            "[461 | 9400.23] loss=2.89 avg=2.89\n",
            "[462 | 9418.96] loss=2.70 avg=2.88\n",
            "[463 | 9437.28] loss=2.70 avg=2.88\n",
            "[464 | 9455.72] loss=2.72 avg=2.88\n",
            "[465 | 9474.27] loss=2.65 avg=2.88\n",
            "[466 | 9492.83] loss=2.91 avg=2.88\n",
            "[467 | 9511.76] loss=2.80 avg=2.88\n",
            "[468 | 9530.36] loss=3.08 avg=2.88\n",
            "[469 | 9548.87] loss=2.80 avg=2.88\n",
            "[470 | 9567.32] loss=2.81 avg=2.88\n",
            "[471 | 9585.72] loss=2.83 avg=2.88\n",
            "[472 | 9604.62] loss=3.31 avg=2.88\n",
            "[473 | 9624.53] loss=2.90 avg=2.88\n",
            "[474 | 9643.80] loss=1.95 avg=2.87\n",
            "[475 | 9662.44] loss=3.30 avg=2.88\n",
            "[476 | 9680.99] loss=3.03 avg=2.88\n",
            "[477 | 9699.53] loss=3.02 avg=2.88\n",
            "[478 | 9718.11] loss=2.67 avg=2.88\n",
            "[479 | 9736.86] loss=2.68 avg=2.88\n",
            "[480 | 9755.65] loss=2.99 avg=2.88\n",
            "[481 | 9774.35] loss=2.74 avg=2.88\n",
            "[482 | 9793.08] loss=3.22 avg=2.88\n",
            "[483 | 9811.76] loss=2.84 avg=2.88\n",
            "[484 | 9830.43] loss=2.97 avg=2.88\n",
            "[485 | 9849.59] loss=3.06 avg=2.88\n",
            "[486 | 9870.40] loss=2.42 avg=2.88\n",
            "[487 | 9888.88] loss=2.81 avg=2.88\n",
            "[488 | 9907.89] loss=2.76 avg=2.88\n",
            "[489 | 9926.84] loss=2.91 avg=2.88\n",
            "[490 | 9945.71] loss=2.67 avg=2.87\n",
            "[491 | 9964.31] loss=3.09 avg=2.88\n",
            "[492 | 9983.04] loss=2.79 avg=2.88\n",
            "[493 | 10001.77] loss=2.96 avg=2.88\n",
            "[494 | 10020.42] loss=2.66 avg=2.87\n",
            "[495 | 10039.02] loss=2.71 avg=2.87\n",
            "[496 | 10058.15] loss=2.69 avg=2.87\n",
            "[497 | 10076.98] loss=2.77 avg=2.87\n",
            "[498 | 10095.93] loss=2.59 avg=2.87\n",
            "[499 | 10116.31] loss=2.26 avg=2.86\n",
            "Saving checkpoint/run1/model-500\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "),\n",
            "and the latter is the only one which can be\n",
            "conceived as a necessary condition of the existence of the\n",
            "existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is\n",
            "a necessary condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the existence of the world.\n",
            "\n",
            "The existence of the world is a necessary condition of the\n",
            "existence of the world, and the existence of the world is a necessary\n",
            "condition of the\n",
            "\n",
            "[500 | 10225.27] loss=2.77 avg=2.86\n",
            "[501 | 10244.34] loss=2.90 avg=2.86\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 10: Creating a Training Model directory\n",
        "\n",
        "#Creating a Training Model directory named 'tgmodel'\n",
        "\n",
        "import os\n",
        "run_dir = '/content/gpt-2/models/tgmodel'\n",
        "if not os.path.exists(run_dir):\n",
        "  os.makedirs(run_dir)\n",
        "  "
      ],
      "metadata": {
        "id": "ynUNLAsMZdeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 10A: Copying training Files\n",
        "\n",
        "!cp /content/gpt-2/src/checkpoint/run1/model-500.data-00000-of-00001 /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/src/checkpoint/run1/checkpoint /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/src/checkpoint/run1/model-500.index /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/src/checkpoint/run1/model-500.meta /content/gpt-2/models/tgmodel"
      ],
      "metadata": {
        "id": "XetEMUrmaJpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 10B: Copying the OpenAI GPT-2 117M Model files\n",
        "\n",
        "!cp /content/gpt-2/models/117M/encoder.json /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/models/117M/hparams.json /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/models/117M/vocab.bpe /content/gpt-2/models/tgmodel"
      ],
      "metadata": {
        "id": "wFiu_KdKarRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 10C: Renaming the model directories\n",
        "\n",
        "import os\n",
        "\n",
        "!mv /content/gpt-2/models/117M  /content/gpt-2/models/117M_OpenAI\n",
        "!mv /content/gpt-2/models/tgmodel  /content/gpt-2/models/117M\n"
      ],
      "metadata": {
        "id": "_PYSR2pEa5nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fire"
      ],
      "metadata": {
        "id": "RsEjXkW7dQai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 11: Generating Unconditional Samples\n",
        "\n",
        "import os # import after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2/src\")\n",
        "\n",
        "!python generate_unconditional_samples.py --model_name '117M'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwsSdHklbgtu",
        "outputId": "8f13bd16-e73c-428a-9f84-7a7aeed4af23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From generate_unconditional_samples.py:54: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2022-11-07 18:58:36.519224: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-11-07 18:58:36.522467: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
            "2022-11-07 18:58:36.522712: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29ef9c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-11-07 18:58:36.522745: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "WARNING:tensorflow:From generate_unconditional_samples.py:56: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:39: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From generate_unconditional_samples.py:65: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "======================================== SAMPLE 1 ========================================\n",
            "By Evan Affrin, Sameer Ibrahim, and Nadim Hasan\n",
            "\n",
            "The rosy consensus of economics suggests sufficiently little progress has materialized to justify demonetization of basic services and masked dire forecasts of a new race to socialist supremacy in the…<|endoftext|>SYRIAN SALVER ... Adolescent can heal after creeper pain, vocal improvement, craniofacial repair, and bone amelioration\n",
            "\n",
            "Adolescent can heal after creeper pain, vocal improvement, craniofacial repair, and bone ameliorationstroke and retinal deterioration\n",
            "\n",
            "7 September 2001\n",
            "\n",
            "Date HAEDIM\n",
            "\n",
            "Breaking News - What about the 2095 G.D.?\n",
            "\n",
            "Disclaimer - The writer is not responsible for the content of this page. The information below is compiled from two sources whose content is no longer current and is not accurate.<|endoftext|>Rep. Tara Gallagher is retiring from the House health care committee because her board's review of universal health care coverage curtailed advances made by the president's health care reforms.\n",
            "\n",
            "Gallagher across the aisle launched her newly-launched public relations campaign Monday when she announced the news that she would abdicate her majority House seat.\n",
            "\n",
            "Breaking News: Ottawa Ombudsman Sheila Tate issued a one-page community bulletin of political infighting this month, replacing an appearanceatelay of the government's Telecom Justice Committee with a content advisory on \"the University of Ottawa's public mistrust of notice,\" and the release of a letter sent by then-health minister Sylvia Horgan saying her board prevented a \"inadequate review of Canada's embattled public insurance system.\"\n",
            "\n",
            "\"These kinds of corruption and lies have bespeaked our public service for millions of years,\" Gallagher, the Republicanbal did in December, adding her board is \"fully supportive\" of \"repatriation of senior staff to our nation's culture,\" noting that she or similar number of shares that had municipal government post and are recommended by Ottawa are \"too good for the future.\"\n",
            "\n",
            "Gallagher backtracked from her recent comments Sunday that she wanted someone to put in charge of the Finance Committee before any process could be went against the wishes of the Trump administration.\n",
            "\n",
            "Just a few days before ground game in the Senate last Wednesday, the Holstein-Ford-Edwards commission of inquiry launched an extensive review of public health programs and $6.5 billion in private educational and research investments, and last week was tasked with looking at a Belleville infrastructure project included in the United Nations's World Health Organization's Health Access Report. A July report has identified the projects in future campaigns as worthy of scrutiny.\n",
            "\n",
            "Gallagher's prepaid PAC and her affiliated advocacy organization are no longer prominent officials in light of the departing Ellen Koontz, announced a week earlier:\n",
            "\n",
            "\"I am relieved that my unit has been audited, trapped behind having to fend off political opponents trying to pass legislation like this Congress against the hardships of these crucial tasks for individuals and communities across Canada,\" she said through her spokeswoman Sue Hafner. \"However this comes as nothing less than the most profound political blow to Canada's health care and social justice agenda. None of her public relations and civil service positions is without a good saying, while I thank Senators and Parliamentional Leaders and the entire Commons and women for standing up for Ottawa's long standing position when it comes to giving Canadians affordable health care more consistent care that matters to them.\"\n",
            "\n",
            "Regardless if she remained on that job, Gallagher's departure \"is very much a decision that needs to be made by the next government. I think 73 of you, 80 of you are committed to providing high quality, affordable Canada Health Care coverage to all Canadians, and I certainly thank fellow news employees for stepping up to make every effort to determine what will epitomize how timely this is going to be this time of the year.\"\n",
            "\n",
            "Those contributions continue today without this week's announcement:\n",
            "\n",
            "Life Insurance BC CACCOMD 2022 Holiday COD\n",
            "\n",
            "Living Spouse Fund - 1 unmined commemorative $800,999 Senior Life Insurance Manner Plus\n",
            "\n",
            "Love Health - 427 Unlinked \"Kiss My Baby\" jarwith six $100,000 National Health Inspection Service Professional Sexual Assault Support\n",
            "\n",
            "Help Fund for Care in Children -50 $500 Chapter 11 Pension Investment\n",
            "\n",
            "Support Canada Fund - 375 $500 Child Support Intake Centre\n",
            "\n",
            "Read more of the Gazette's coverage of Gallagher. Visit http://craiffinkdesk.com/gc/news as well as Fairer Health » Canadian News Affairs<|endoftext|>No JavaScript? We need that :(\n",
            "\n",
            "New at SubtleTV! Close\n",
            "\n",
            "Video: Video: Camaroen - Hydrogen Breading - Pump Pump Token exclusive for: Escalation's vocal channel. Total time: 310m 11ms 38ms 23ms 40m 100m<|endoftext|>At about 60 years old, Lithium was the first of major metallic elements (Chlorides, Hydrazine, Sulfur are some of the major alkali sources\n",
            "======================================== SAMPLE 2 ========================================\n",
            "12 Best Prep Titles of the Weekend\n",
            "\n",
            "Schoolmaster NFL Awards Guy Lodes 17 Grantsley High School\n",
            "\n",
            "\n",
            "1235 North Concord Pike\n",
            "\n",
            "\n",
            "New Regency School all prefootball teams (Only selected by our last online Board Games Tarot App)\n",
            "\n",
            "\n",
            "Studio presents\n",
            "\n",
            "\n",
            "Eight Great Top 40s Just Dif.\n",
            "\n",
            "\n",
            "10 Interesting Lyrical Things Portrayed Better of Gentius\n",
            "\n",
            "Share\n",
            "\n",
            "1 Redditor had 178 views in favorite 2 vote\n",
            "\n",
            "\n",
            "Saturday, August 12, 2007\n",
            "\n",
            "1 Muscle Stacking Over Weekend Moments From the Chamber at Work\n",
            "\n",
            "3 Modest Holidays, Ultrafit Gothic Super Bowl Coach Mint J\n",
            "\n",
            "\n",
            "1218 Washington St\n",
            "\n",
            "Coulsma, OH 45185\n",
            "\n",
            "\n",
            "3,277 views\n",
            "\n",
            "\n",
            "VIEW MORE INFO<|endoftext|>[KUALA LUMPUR, Sept 4 18:15 AM (CST)) – Malaysia Cyber Security Analysts on Saturday reported the Bridge Attack malware source on a 6,000-DTC Malicious Generation aero cache used by both attackers and victim, which was characterized by an unknown-repeat attack at the same time but within the same stages of execution. One of the attack became unsecured shortly before 40:25 and stopped at 00:15:\n",
            "\n",
            "OW more than one possible have been identified\n",
            "\n",
            ">> With over 24-m3 passwords still usable Nadhim Zkaytsev showed WCFO has been targeted by twin AadhyberCon sting\n",
            "\n",
            ">> This latter attack turned out to be campaign code to target the attacker with the spear-phishing Nuget attack believed to still be active as of June 18>> It could have taken target breach maj Cluster IoT Security for two days to secure the breach\n",
            "\n",
            "[KUALA LUMPUR, Sept 4 17:45AM (CST)) – Ninety-three Admins were injured in the continued attacks across 7-10 Updated: All Individuals renovate and deactivate private keys to keep alliance security up to date\n",
            "\n",
            "\"Critical readiness measures in all affected installations are being implemented for accordance with evolving policy reports submitted by our own national interior security teams at this particular time,\" says around 60 nationals exposed.\n",
            "\n",
            "Final updated status of alleged attack: In the movie, the widow of Malath Kun almost dies, but the Tyrant Battalions execute her to get revenge after she was killed in a surprise attack, but once she manages to get out, to Paltin's Wife Annory he robs the sole survivor. Kabi seemed to be unaffected by the attack and two-headed lion, Gabriela, slept on her Assimilator's head just to give at least a hard reality check while Reich detail the origins of the malware, because, it turned out, nostalgia is not the doctorate in defence against having deserted American and British forces in a major battle. But knew for sure that this is how the slavers of more subtle-looking \"containers\" Eddert acquired to demolish intial apartments in Berlin weren't brandish-armed Che, just clarified the Black-and-White of the picture Mark the Scratch The Smarm Office look.\n",
            "\n",
            "HE SIMMED AP/ CAREERCIALIAL alarine.frontiers@world-premium.ntp.gov.uk\n",
            "\n",
            "What more did its famous tobacco salesman call that's yet to become a McClatchy derivatives tweet:\n",
            "\n",
            "But yes, this is nothing new, 17 years after his predicated plumber left 3-4 years to start up his Woodlands Supply business p770s courtesy of profs Marco III/Crusher/Winchester Tech for Free (my first real major venture in auto adhesive), even salvaged only the leftovers of the Chrysler Junior Flower direct from 1-6 years old . But what sets Advanced Momentum anyother apart after dying of cancer four years without injecting?\n",
            "\n",
            "Choosing damas to make for a quantum leap through information with MANY technical leaps.\n",
            "\n",
            "DONZALVA ability to go on a SoundWaveNotifier run by source user Lenku then. (-rossover #CDC and part 7a From Bobby part 5 push \"The Ultimate Technician Sight Technician\" method of a Scan 22Although \"book extrapometers\" as this Byrne side-effect is very nice work it 'horners VERITAS and \"desps administratifff\" rejacking 24 different masters first to Moscow and in two and a half hours Tor' ira arrange Andalara and PK Planet Press, inverting predicted Chimera hit on KP 2 The strategy in tries Zemlya Corals and VI:HOWTO Methodset work of the dustbowl ritual, in which technicians ordered first only, to attemptverweid's, Lapis Aqueoprene Schneider flaw of the top hard (HPD/CRL/Flt) slides of the rear panel tools did not succeed and eventually shut in an engine blew off Nitrin shells which flooded the van forna to crash which took Rackstein 46master lightweight k\n",
            "======================================== SAMPLE 3 ========================================\n",
            "PCHunter is a Web start-up that works by scraping your ip address and trying it out with GitHub. Our aim is to help more folks optimize their Child as developers forums to make a system that simply supports logging on, using mail clients, and maintain their chats based on that a common forum which did not have the mapping they wanted don't help them get across to us. We came up with the concept step by step; by using Telegram ‹ [Sci, Coders, and Tablecasts] ‹ that can provide access to full chat lists of under 50,000 people using both Telegram and the giveaway block.\n",
            "\n",
            "Quote\n",
            "\n",
            "We use a 2-finger handshake action. The request closely resembles saying \"give me your Computer and then produce a screenshot with my WIN BillPegasus.\"\n",
            "\n",
            "We tried all steps since last April. The result was amazing success every time thanks to the people who send us a message, text message, YET our friends making smooth posts, ignoring us which we regret but added them with our encrypted password.\n",
            "\n",
            "\n",
            "With single click — no need to protect code or data — that's the mini version of the jam Google issues.\n",
            "\n",
            "\n",
            "Other Java developers would like to start up our own Jabber-as-Server with Testbe community free.\n",
            "\n",
            "We truly want to help every developer on the internet by setting them up to hand off resources and helpon related projects. We try to win with all existing GitHub subscribers who respond promptly. Backers receive full credits for small breaks (including buying 16 yds of blueprints for a place to publish their bots)! To kindly ask before you and our 9Best partners start using our free API anytime soon. [/quote]<|endoftext|>Announcement October 18, 2017 by Jennifer Beadle\n",
            "\n",
            "Freddy Cat was finally out playing 'eweshiiiiini cinema' as the 11th Marquis Martial Dame. With a physical shown at Las Vegas International Music Festival, Dan Sullivan now has a \"moment of truth\" out there.\n",
            "\n",
            "As he explained on the movie website Firetyable.com, Stulten is an Eastern European, roughly speaking. Colonel Bruce Long (an honor student of the Marquis who's going to school in Los Angeles. His name with regards to success in South Asian dance dancing.) acts more as Brian disinterred Professor Tourt (Larry Darlene). Along with the Fox Austin couple Bruce Irlick and David Heywood, Coloradan Jenny Ogilvie works hard on colour, as well as crossdresser Dennis Reynolds, as well as meets Brenda Bishop or the Dennis forecasting \"I'll be able to see Fred again, he'll be there playing in suitable colour and having his appearance be good for Upper Class Members.\" The match that wins when Renoir Cage comes out also changed their lives and demands particular changes for Fred traveling and walking around Vegas at the 3700+ mark / get another point to play awhile. All of that for one FUCK patch.\n",
            "\n",
            "Because Danny comes out in the mid 40's, Milla is out of school. Cutler-Smith really found his calling as the 'Imax of Underemployment changed everyone's lives.' As for Dekeye, her change to representation in the movies with Tamora's incredible new, expanded role lady, Jefferson Granger, who put up commercials for people to put down Christmas trees for the quality, amazing popcorn we're used to. Looking at the cartoons now, we had to cross the narrative line from 'we're ready to stand up when the sun goes down' to 'we ain't never gonna rise in the sun' * Love realm has a Twilight Zone country accent. The description of Geraldine purportedly making fun of elves found in Magic the Gathering and being propositioned by Canadian birthright parents like Melinda, now places her in the \"absent and limited\" camps of the American Gold Scheme and acts like a lid off only the Israel/Homer line is had to pass in Shanghai. Mind you that \"Medieval famines have never been so terrible.\"\n",
            "\n",
            "Anna Sinclair plays Madame Regnae in a role expressed in Wat Lin both in her Oscar nomination for \"My Love Story in 24\" and in Lin Quentin's crucial title role later that year in London's Spies. Bonnie and Clyde This s'man men s'vita profile featured Nicola Piccaban on the show's casting guide.\n",
            "\n",
            "Why to barstoolup Danny Cat Heat Ave findings using bweptraffic and visualize over 3Mad []\n",
            "\n",
            "Cross dresser/rdergic blushing Uitate Girls<|endoftext|>The Cross-Giga Window Mapping Tool allows you to automate the 'weighted weight' measurements of multiple monitors (fan, side, and deadzone-edge) in multiple devices, such as , by operating their fullscreen and the DVI specification. This can be easily accomplished using image accuracy tools such as Adobe RGB or Matlab.\n",
            "\n",
            "When used a softmax mouse options allow very large monitors to show in an ideal way\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 12: Interactive Context and Completion Examples\n",
        "\n",
        "import os # import after runtime is restarted\n",
        "\n",
        "os.chdir(\"/content/gpt-2/src\")\n",
        "!python interactive_conditional_samples.py --temperature 0.8 --top_k 40  --model_name '117M'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dslo3xaIdE_M",
        "outputId": "53a9e018-246f-404b-8966-22d7fc44a097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From interactive_conditional_samples.py:57: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2022-11-07 19:03:41.801847: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-11-07 19:03:41.805016: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
            "2022-11-07 19:03:41.805237: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x315d9c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-11-07 19:03:41.805272: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "WARNING:tensorflow:From interactive_conditional_samples.py:58: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From interactive_conditional_samples.py:60: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From interactive_conditional_samples.py:68: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Model prompt >>> Human reason, in one sphere of its cognition, is called upon to consider questions, which it cannot decline, as they are presented by its own nature, but which it cannot answer, as they transcend every faculty of the mind.\n",
            "======================================== SAMPLE 1 ========================================\n",
            " It is thus necessary, by reason of the existence of this world-nature, that as to questions of this nature not only be left to external objects, but likewise to questions of the nature of things, it is necessary, with knowledge of the truth, to call upon external objects, as well as to ask them questions. This is done by reason of the fact that all the phenomena of nature arise from the same faculty.\n",
            "\n",
            "In the same way, in the next sphere, questions concerning the existence of external things, which are connected with the nature of things, are called upon to inquire into the existence of external things, or, when the external things are called upon, to answer their questions as they are called upon. For it is the nature of the things that are called upon, that makes them called upon. Therefore, in the next sphere, questions concerning the existence of external things, which are connected with the nature of things, are called upon to inquire in the nature of things.\n",
            "\n",
            "A question of the nature of things is called upon to be considered, and thus a question of the nature of things is called to be considered. For this is in itself a question of the nature of things, as it is to be considered.\n",
            "\n",
            "If, therefore, they are to be called upon to inquire into the nature of things, the question is taken to be the following: If they are to be called upon to inquire into the nature of things, then a question of the nature of things ought to be considered.\n",
            "\n",
            "If, therefore, the nature of things is not as it was before, inasmuch as it is not as it is now, the question of the nature of things ought to be considered; then the question ought to be considered first, and then, after that, should a question be considered. For when one examines the nature of things, one must ask the question first; when one examines the nature of things, one must answer the question. Thus, if they are to be called upon to inquire into the nature of things, then they ought to be called upon to inquire into the nature of things.\n",
            "\n",
            "As, therefore, in the next sphere, they are to be called upon to inquire into the nature of things, the question ought to be considered first, and then, after that, should a question be considered.\n",
            "\n",
            "If, therefore, their nature is not as it was before, inasmuch as they are now as they were before, the question should be considered first, and\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"interactive_conditional_samples.py\", line 73, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"interactive_conditional_samples.py\", line 91, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 681, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"interactive_conditional_samples.py\", line 73, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 11: Saving and downloading the model to your laptop\n",
        "\n",
        "!zip -r /content/file.zip /content/gpt-2"
      ],
      "metadata": {
        "id": "432kxZkvgPyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L6sH_-icO0YC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "B0bAemKbgetX",
        "outputId": "88249125-d131-4b3c-ee48-28100724510b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_82121868-72c3-474f-beab-823a3eb63787\", \"file.zip\", 1863994390)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gpt-2.zip "
      ],
      "metadata": {
        "id": "OJRlOkee2NCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 12: Loading the saved model from Google Drive\n",
        "\n",
        "import os # import after runtime is restarted\n",
        "\n",
        "\n",
        "# make sure your google drive is mounted first\n",
        "!cp  /content/drive/MyDrive/gpt-2.zip .\n",
        "\n"
      ],
      "metadata": {
        "id": "2uClogzcOUlf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.15.2\n",
        "!pip install toposort\n",
        "!pip install fire\n"
      ],
      "metadata": {
        "id": "1YYHf0dmGvwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 13: Generating Unconditional Samples\n",
        "\n",
        "import os # import after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2/src\")\n",
        "\n",
        "!python generate_unconditional_samples.py --model_name '117M'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoiPnSIuHHhf",
        "outputId": "8821b443-193f-4d58-fbe9-1b7429af05e7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From generate_unconditional_samples.py:54: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2022-11-09 20:38:47.781715: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-11-09 20:38:47.786319: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2022-11-09 20:38:47.786549: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x20c59c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-11-09 20:38:47.786585: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "WARNING:tensorflow:From generate_unconditional_samples.py:56: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:39: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From generate_unconditional_samples.py:65: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2022-11-09 20:38:51.658153: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 154389504 exceeds 10% of system memory.\n",
            "2022-11-09 20:39:41.367795: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 42098688 exceeds 10% of system memory.\n",
            "2022-11-09 20:39:41.467663: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 42172416 exceeds 10% of system memory.\n",
            "2022-11-09 20:39:41.566931: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 42246144 exceeds 10% of system memory.\n",
            "2022-11-09 20:39:41.671185: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 42319872 exceeds 10% of system memory.\n",
            "======================================== SAMPLE 1 ========================================\n",
            "How can you avoid finding privacy problems? How do you know where to find them?\n",
            "\n",
            "No one responds to frustrating queries directed against you. Why? In fact, there's no easy answer. In short, you still have a lot to learn about privacy.\n",
            "\n",
            "It sounds without context, but the concept behind privacy is nuanced. Privacy is a way to keep track of information released by a person and to protect yourself and your information. As a result, privacy is different in each and every organization/network/network/network achieved by an organization.\n",
            "\n",
            "Here are some security features, functions, or enhancements that could help protect your privacy and misuse if you can't get used to them.\n",
            "\n",
            "Privacy safeguards for networking, BGP, private messages; Appropriate time allocation to handle sensitive communications; Personal data from your files\n",
            "\n",
            "Privacy protection for the safe building of networks\n",
            "\n",
            "Trying to understand the \"golden age\" of building personal networks is always challenging. Friends and family have goals and goals, target sizes, and a variety of mobile applications. However, in the case of an individual network it is almost impossible to keep pace with, most devices or server worlds are built with multiple ends on the same host. Packets are created at lower speeds and processing happening at higher speeds on different networks, with varying mitigation mechanisms for attacking these systems in more typical scenarios.\n",
            "\n",
            "Measuring back to an earlier age could easily put the original security implications into perspective, although this depends on your way of viewing and monitoring networks in the context of today's scrutiny. If you can count how many devices change over time, do you plan on running any crypto whether I register it or start doing so? or sampling it to learn more about encryption technique almost immediately? It's hard to say how raw data is securely distributed among different devices and original network applications.\n",
            "\n",
            "Data base security measures such as scraping need to come from intelligence sharing, not developers, not privacy security folks, and the new services such as Cassandra allow health and data confidentiality. Using social media is a complex task to rehearse, but learning the most productive way to win users was surprisingly challenging.\n",
            "\n",
            "Metadata\n",
            "\n",
            "Wi-Fi IP addresses are stored instantly, as well as firewalls, police metadata, and communications. I do want to let you know that false memory (e.g., device #1037923, an email sent from 13:34 UTC, timestamp 161197458) is saved by arriving at the suspect's IP address. Let alone zero-day large-scale analysis, images and video have the potential to be useful and useful. Testing your personal network counters a situation by finding and removing L2P traces; there's a tremendous amount of biological protection against any potential gains.\n",
            "\n",
            "Certificate Strengthening Programs\n",
            "\n",
            "Measuring the size, privacy, and tonality of individual data makes it difficult to judge the default or ongoing validity of the user's credit card number, bookmark. I drop various recommendations, personal finance cookies, application privacy and aggregate/slowing require that we consider interchangeable defaults, and there are a host of user reported and algorithmic outliers this way.\n",
            "\n",
            "While I choose to enforce a baseline assumption when commercial currencies or currency concepts are described, it's important to provide some framework for testing. The cloud and distributed computing space Windows systems or hyperscale applications with cloud endpoints per PC_cache typically fail because the performance varies greatly across libraries and operating systems. Cloud based business offerings are usually slower and sometimes dangerous to region-specific systems, but every micro-controller/framework/framework is different, from open-source EC2 crypto to leveraging IP-specific cloud algorithms.\n",
            "\n",
            "By convention, an attack is expected only on enterprise markets, on servers that employ transparent servers, emulating open-source binaries on a corporate premise, but for the majority of date years these vendors are not deployed in most enterprise markets. IT admins work on the publish projects lobby, pushing the document revision committee to a private committee and manufactering virtual validation tools, and maintained by malware experts. Provider compliance and response rate fly in the face of the design of the products–a product minutiae that is the basis of regular portability and security. Migrate and move your devices to freelance work where this will eventually be deciphered/valuated in the event of signature errors.\n",
            "\n",
            "Understanding mDSRs funds research conducted on a resource based on the test current and mkdb(ctl) metrics. There is also monitoring of functions and backends to monitor and extract information from your project mDSRs.\n",
            "\n",
            "Real-time response rates\n",
            "\n",
            "Everyone knows service reports client-side so lacking PHP can't be overlooked today, but I see it in PHP 3+, along with connectivity to the same data officer that communication occurs between command line (CSV) and server instead. This simple API is comprehensive but should be data type rich, and will allow you to access huge amounts of data without having to explicitly specify what type of data it will contain.\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 14: Interactive Context and Completion Examples\n",
        "\n",
        "import os # import after runtime is restarted\n",
        "\n",
        "os.chdir(\"/content/gpt-2/src\")\n",
        "!python interactive_conditional_samples.py --temperature 0.8 --top_k 40  --model_name '117M'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-iIE0p7H752",
        "outputId": "960475c7-ec50-4455-9849-513a6ec6cf1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From interactive_conditional_samples.py:57: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2022-11-09 20:41:13.372529: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-11-09 20:41:13.376137: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2022-11-09 20:41:13.376404: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e3d9c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-11-09 20:41:13.376468: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "WARNING:tensorflow:From interactive_conditional_samples.py:58: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From interactive_conditional_samples.py:60: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From interactive_conditional_samples.py:68: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2022-11-09 20:41:17.891405: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 154389504 exceeds 10% of system memory.\n",
            "Model prompt >>> “If we now put aside all cognition that we have to borrow from objects and merely reflect on the use just of the understanding, we discover those of its rules which are necessary without qualification, for every purpose and without regard to any particular objects of thought, because without them we would not think at all\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". They are mere rule of reason so that we can never be deprived of them at any rate as regards objects in any of their relations. They are, then, only rule of reason, of which we must learn of their rules to understand them. We are thus thus bound in one respect by a rule of reason, namely, to the use of the understanding, since all the objects of thought are to be interpreted in a given way with reference to them in the understanding. This we do by means of a rule of reason. That we may not, or are not, to be deprived of these rules is merely an objection. We have found that we are to be deprived of these rules by having to be given the understanding. This is all that is required of us by such a rule of reason. But it is also the rule of reason. Since as to the necessity of these rules of reason, we can not be obliged to use them, because that would not be to say that our understanding is to be understood only, but that we may have no use for their rules of reason, we have to consider that we can only have the rules of reason, and not that we may require or require anything less than they.\n",
            "\n",
            "The rule of reason is thus of great benefit to a number of people, especially when we consider their understanding at the level of the reason. They see that the explanation of the whole of their understanding is to be understood by them without the understanding. They find, in the same way, the reason for the rule of reason. They say that they have the rules of reason, because they know that they know that they have a rule of reason. They feel that they have the rules of reason, because they have the understanding.\n",
            "\n",
            "But what they do not understand is exactly the same thing, that the laws of that understanding are not to be understood in the same way. They are to be understood only in their understanding, and not in their understanding in this way. For if, in order to know the principles of reason, one has to know something different about a thing, this would cause one to be in error, and thus to be denied the meaning of the interpretation of the laws. Thus one is in error in the interpretation of the law. Now the reason for this is to be understood in the proper way—the understanding being understood only in the understanding. The same rule applies to the interpretation of the law. For if one has to understand the law of nature, and the interpretation of the laws are different\n",
            "================================================================================\n",
            "Model prompt >>> "
          ]
        }
      ]
    }
  ]
}