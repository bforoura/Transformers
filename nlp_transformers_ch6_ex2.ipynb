{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPHgyGfuTzfDMpdG6f2FPbk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bforoura/Transformers/blob/main/nlp_transformers_ch6_ex2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 1: Activate the GPU\n",
        "\n",
        "# Go to Runtime / Change runtime type / Hardware Accelerator GPU\n",
        "\n"
      ],
      "metadata": {
        "id": "2cGmiykKtvqC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oycray6wtY2T",
        "outputId": "c55742e2-bca3-4479-877f-fa730016902d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 233, done.\u001b[K\n",
            "remote: Total 233 (delta 0), reused 0 (delta 0), pack-reused 233\u001b[K\n",
            "Receiving objects: 100% (233/233), 4.38 MiB | 13.11 MiB/s, done.\n",
            "Resolving deltas: 100% (124/124), done.\n"
          ]
        }
      ],
      "source": [
        "#@title Step 2: Cloning the OpenAI GPT-2 Repository\n",
        "\n",
        "!git clone https://github.com/openai/gpt-2.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 3: Installing the requirements\n",
        "\n",
        "\n",
        "# import os          # when the VM restarts import os necessary\n",
        "# os.chdir(\"/content/gpt-2\")\n",
        "# !pip3 install -r requirements.txt\n",
        "\n",
        "# The requirements have been installed automatically.\n",
        "\n",
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.15.2\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jd5eySuXvomX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f2ea81-feed-4a38-b597-60a9ecb026c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.9.2\n",
            "Uninstalling tensorflow-2.9.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.9.2.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.15.2\n",
            "  Downloading tensorflow-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 37 kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.21.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 70.2 MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 54.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.14.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (2.0.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.37.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.50.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.2) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=d5f6efd8fa5dfa714c427cf56090c8f5a32b1351e1fdbc781179e843a1776059\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 4: Checking the Version of TensorFlow\n",
        "\n",
        "#Colab includes Tensorflow 2.x by default but we need a lower version\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6diUYlpyYWp",
        "outputId": "1b9fa297-23ec-4215-f6fb-50d2e33c972a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 5: Downloading the 345M parameter GPT-2 Model\n",
        "\n",
        "# run code and send argument\n",
        "import os \n",
        "\n",
        "os.chdir(\"/content/gpt-2\")\n",
        "!python3 download_model.py '345M'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dnJLPePzRV7",
        "outputId": "e9010094-5839-4c8e-9272-43956778fb0f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 945kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:01, 632kit/s]                                                    \n",
            "Fetching hparams.json: 1.00kit [00:00, 761kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [04:53, 4.83Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 5.44Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:01, 496kit/s]                                                  \n",
            "Fetching vocab.bpe: 457kit [00:01, 308kit/s]                                                        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 6: Printing UTF encoded text to the console\n",
        "\n",
        "!export PYTHONIOENCODING=UTF-8\n"
      ],
      "metadata": {
        "id": "LFvtQWWM28QT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 7: Project Source Code\n",
        "\n",
        "\n",
        "os.chdir(\"/content/gpt-2/src\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Sgr7Mk3C3CpR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 7a: Interactive Conditional Samples (src)\n",
        "\n",
        "#Project Source Code for Interactive Conditional Samples:\n",
        "# /content/gpt-2/src/interactive_conditional_samples.py file\n",
        "\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "metadata": {
        "id": "Z8kYpra53Jyh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Steps 7b-8: Importing and defining the model\n",
        "\n",
        "\n",
        "import model, sample, encoder\n"
      ],
      "metadata": {
        "id": "qX-q1CXu3Tmy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 8: Defining model interaction\n",
        "\n",
        "def interact_model(\n",
        "    model_name,\n",
        "    seed,\n",
        "    nsamples,\n",
        "    batch_size,\n",
        "    length,\n",
        "    temperature,\n",
        "    top_k,\n",
        "    models_dir\n",
        "):\n",
        "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
        "    if batch_size is None:\n",
        "        batch_size = 1\n",
        "    assert nsamples % batch_size == 0\n",
        "\n",
        "    enc = encoder.get_encoder(model_name, models_dir)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if length is None:\n",
        "        length = hparams.n_ctx // 2\n",
        "    elif length > hparams.n_ctx:\n",
        "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        np.random.seed(seed)\n",
        "        tf.set_random_seed(seed)\n",
        "        output = sample.sample_sequence(\n",
        "            hparams=hparams, length=length,\n",
        "            context=context,\n",
        "            batch_size=batch_size,\n",
        "            temperature=temperature, top_k=top_k\n",
        "        )\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "\n",
        "        while True:\n",
        "            raw_text = input(\"Model prompt >>> \")\n",
        "            while not raw_text:\n",
        "                print('Prompt should not be empty!')\n",
        "                raw_text = input(\"Model prompt >>> \")\n",
        "            context_tokens = enc.encode(raw_text)\n",
        "            generated = 0\n",
        "            for _ in range(nsamples // batch_size):\n",
        "                out = sess.run(output, feed_dict={\n",
        "                    context: [context_tokens for _ in range(batch_size)]\n",
        "                })[:, len(context_tokens):]\n",
        "                for i in range(batch_size):\n",
        "                    generated += 1\n",
        "                    text = enc.decode(out[i])\n",
        "                    print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "                    print(text)\n",
        "            print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "WiGK9ZeJoxLS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 9: Interacting with GPT-2\n",
        "\n",
        "interact_model('345M',None,1,1,300,1,0,'/content/gpt-2/models')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pLYMu2GoZN-",
        "outputId": "2c8ae298-629c-48de-cb31-54a0f786e794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:39: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model prompt >>> Human reason, in one sphere of its cognition, is called upon to consider questions, which it cannot decline, as they are presented by its own nature, but which it cannot answer, as they transcend every faculty of the mind.\n",
            "======================================== SAMPLE 1 ========================================\n",
            " Even in the East, reason shudderes at reproaches on its own hypothesis, by which it presupposes the annihilatory power of deity. In the just sense, that is absolutely useless to it; and leaves it to formulate to the infinite the true ideas of man. Enquire through our reason, what, when I conceive this arbitership, is its so far surrendered sovereignty? or I put my theologian if I suppose it yet under the dominion of providence, as the only author which can sanction it; or how hurtful is all these veils that we have rubbed in our conclusions! \"By the power, \" is that exceedingly touching phrase; \" gave me—to want it,—bought that power up;\" \"princes of times past,\" to whom all banners of times past were sacred. Not then nature, that profession denigrates the promptitude of man to reason: on the contrary, nature, that parent of the thinker, makes its agency responsible to his will, and above all to reason—tends to commence with the sense,—that all reasoning is for pleasure, and can as well proceed, in bestowing may rest on the inquiries undertaken, as on those made with respect to reasons in hand.\n",
            "\n",
            "But to wit: I tell you by experience ourselves, that in such inclinations as these we have sometimes pull aside, luxuriously, almost never order, the senses, and then as often after they stop at the law of nature we\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}